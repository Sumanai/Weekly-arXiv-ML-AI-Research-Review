### Приблизительный план

1. Аннотация (RU/ENG)
2. Введение и мотивация
3. Связанные работы
4. Постановка задачи и границы
5. Данные и сбор
6. Предобработка ЭЭГ
7. Сборка датасета и разметка
8. Метод: обзор пайплайна и модели
9. Обучение и настройка
10. Инференс и интерфейс вывода
11. Оценка и протокол экспериментов
12. Результаты
13. Воспроизводимость и артефакты
14. Ограничения и угрозы валидности
15. Заключение и будущее
16. Ссылки
17. Приложения

---

# EEG→Text: воспроизводимый конвейер восстановления семантики внутренней речи из сигналов ЭЭГ (обзорная версия)

**Автор(ы):** *Вербецкий Эдуард Игоревич*

**Аффилиация:** *Московский авиационный института (национальный исследовательский университет)*

**Контакт:** *verbasik@gmail.com*

**Версия препринта:** v1 (обзорная)

---

## Аннотация

Мы представляем воспроизводимый end‑to‑end конвейер «EEG → Текст» для исследования внутренней речи в парадигмах *read* и *imagine*. Конвейер стандартизует предобработку (очистка артефактов, референс, ICA‑автомаркировка, адаптивная вейвлет‑фильтрация, нормализация), вводит явные контракты данных и поддерживает многоцелевое обучение энкодера ЭЭГ (CNN+Transformer) с векторной квантизацией и вспомогательными головами (грубые семантические категории, домен *read/imagine*, реконструкция сигнала). Мы формализуем задачу как retrieval по фиксированному словарю фраз и параллельную классификацию категорий/домена, описываем протоколы оценки (within‑session, cross‑session, cross‑subject), меры против утечек, набор метрик (Top‑k, MRR, macro‑F1/BA, калибровка доверия) и структуру «репро‑пакета» для воспроизводимости результатов. Предварительные эксперименты указывают на потенциал VQ и многоцелевой оптимизации к стабилизации и интерпретируемости представлений; в следующих версиях планируется систематическое сравнение с сильными бейзлайнами и перенос между сессиями и субъектами.

---

## 1. Введение и мотивация

Восстановление семантики внутренней речи по неинвазивным сигналам мозга — ключевая задача для BCI‑систем, когнитивной нейронауки и мультимодальных интерфейсов. В отличие от инвазивных методов, ЭЭГ безопасна и доступна, но страдает низким отношением сигнал/шум и межсубъектной вариабельностью. Мы предлагаем практичный конвейер от сырых EDF‑файлов до инференса модели, ориентированный на воспроизводимость, стандартизованные форматы и прозрачную оценку.

**Вклад работы (summary):**

1. Формализуем постановку «EEG→Text retrieval» по фиксированному словарю фраз с параллельной классификацией грубых категорий и домена (*read/imagine*).
2. Описываем стандартизованный пайплайн предобработки с anti‑leakage практиками.
3. Представляем архитектуру энкодера ЭЭГ (CNN+Transformer) с VQ и многоцелевой оптимизацией (контрастивная + классификационные + реконструкция).
4. Фиксируем протоколы оценки (within / cross‑session / cross‑subject), метрики и статистические тесты.
5. Публикуем «репро‑пакет»: код, конфиги, фиксированные версии и чекпоинты (структура и инструкции).

---

## 2. Связанные работы (обзорно)

Исследования по декодированию «внутренней речи» из ЭЭГ можно условно разделить на три направления. Первое — классическая линия работ по распознаванию мысленно произносимых единиц: слогов, букв, слов и коротких команд. Эти исследования демонстрируют, что даже на бюджетных, малоканальных системах возможно выделить устойчивые паттерны, однако точность сильно зависит от количества классов, протокола и межсубъектной вариативности. Например, показано, что на задачах воображаемых команд с 8‑канальной гарнитурой рекуррентные модели (LSTM‑RNN) достигают высокой точности при четырёхклассовой постановке, тогда как многоклассовые сценарии (≈10–13 классов) по естественным причинам приводят к падению метрик, оставаясь при этом статистически выше случайного уровня. Параллельно развивается линия «по‑буквенного» декодирования, в которой ЭЭГ‑паттерны рукописных букв комбинируются с языковыми моделями: такой раздельный подход «минимальные единицы → языковая коррекция» показывает жизнеспособность свободного ввода, но пока опирается на сильную поддержку со стороны LLM на втором этапе.

Второе направление — выравнивание мозговых и текстовых представлений и открытые словари (open‑vocabulary) для EEG→Text. Здесь формулировка часто смещается от жёсткой классификации к retrieval/ранжированию или генерации на основе предварительно обученных языковых моделей. Ряд работ используют двухэтапные архитектуры: специализированный модуль для извлечения EEG‑признаков (сверточно‑рекуррентные/трансформерные слои) и языковой модуль (BART, и др.), дополненный стадией «до‑ или пост‑редактирования» генерации крупной моделью (GPT‑подобной) для повышения беглости и семантической согласованности. На корпусах чтения предложений (например, ZuCo v1.0/v2.0) такие системы сообщают прирост по BLEU/ROUGE и BERTScore относительно более ранних подходов, что поддерживает перспективность открытого словаря и семантико‑ориентированных метрик в оценке.

Третья линия — дискретизация скрытых представлений и векторная квантизация (VQ) в биосигналах. Дискретные коды позволяют стабилизировать латентное пространство, упростить семантическое сопоставление и ввести интерпретируемые «единицы» (коды) для анализа. Практика показывает, что полезно контролировать «живость» кодбука (перплексию, распределение частот, предотвращение коллапса) и сочетать VQ с задачами, которые привязывают коды к осмысленным целям (классификация, реконструкция, контрастивное выравнивание). В контексте EEG→Text это открывает путь к более устойчивым и переносимым признакам, особенно при доменной вариативности (read/imagine).

С позиции предлагаемой работы наш вклад комплементарен указанным направлениям. Мы опираемся на практику retrieval‑формулировки и замороженные текстовые эмбеддинги для семантического якорения, расширяем архитектуру многоцелевыми головами и доменно‑адверсарной регуляризацией, а также используем VQ для дискретизации и анализа представлений (перплексия, частоты кодов). Такой дизайн соединяет сильные стороны «классических» декодеров, открытого словаря и дискретных латентных пространств при сохранении воспроизводимости и строгого анти‑утечечного протокола.

---

## 3. Постановка задачи и границы применимости

Пусть $x\in\mathbb{R}^{C\times T}$ — отрезок ЭЭГ ($C$ каналов, $T$ отсчётов). Требуется:

* (a) **Retrieval** по словарю фраз $\mathcal{Y}=\{y_1,\dots,y_N\}$: найти топ‑$k$ кандидатов по счёту $s(f_\theta(x), g(y))$;
* (b) **классификация** грубой семантической категории $c\in\{1,\dots,K\}$;
* (c) **классификация домена** $d\in\{\text{read},\text{imagine}\}$.

Здесь $f_\theta$ — энкодер ЭЭГ. $g(\cdot)$ — замороженное текстовое отображение фразы в векторное пространство (используется **только** на стороне индекса/оценки сходства, см. anti‑leakage). Фиксируем **корпус фраз** и карту $\texttt{phrase}\to\texttt{coarse}$ до начала обучения и оценки.

**Сценарии оценки:**

* **within‑session:** обучение/оценка в рамках одной сессии субъекта (разные триалы);
* **cross‑session:** обучение в одной сессии, оценка — в другой сессии того же субъекта;
* **cross‑subject:** leave‑one‑subject‑out (LOSO) и/или адаптация ≤5% данных субъекта.

**Ограничения:** не рассматриваем OOD‑фразы вне словаря, синтез аудио/текста и онлайновую адаптацию в v1.

---

## 4. Данные и предобработка

В качестве исходного материала используются сырые записи ЭЭГ в формате EDF с аппаратными метками событий и привязкой каждой эпохи к конкретной фразе. Для унификации обработки мы фиксируем частоту дискретизации на уровне 500 Гц, используем согласованный монтаж и стабилизируем перечень рабочих каналов после исключения артефактных и вспомогательных (например, EOG). В каждом файле дополнительно сохраняется служебная метаинформация: идентификаторы субъекта, сессии и прогона, упорядоченный список имён каналов, хэш-сигнатура монтажа, сведения о «плохих» каналах, если таковые выявлены на этапе сбора.

Предобработка выстраивается как последовательный конвейер, цель которого — повысить отношение сигнал/шум и обеспечить воспроизводимость дальнейших шагов обучения. Сначала применяется полосовая фильтрация с пропусканием диапазона 0.5–50 Гц, затем подавляются сетевые гармоники на частоте 50 или 60 Гц и их кратные. Чтобы снять систематические смещения и аномальные электроды, используется пакет PREP: выполняется ре-референс, детектируются шумные каналы (включая RANSAC‑интерполяции), а решения по исправлению фиксируются. Все параметры PREP обучаются строго на обучающем подмножестве данных текущего фолда и затем неизменными воспроизводятся на валидации и тесте.

Разметка событий извлекается из аппаратного канала триггера и сопоставляется со списком фраз, после чего записи сегментируются в два непересекающихся окна: для домена read — от 0 до 5 секунд и для домена imagine — от 5 до 8.3 секунд. Базовая коррекция не применяется, а использование пост‑событийной информации вне заданного окна исключается. Для очистки выбросов и восстановления испорченных проб применяем AutoReject: пороги оцениваются только на обучающих эпохах, журналы и параметры порогов сохраняются, к остальным разбиениям применяется чистая трансформация без подгонки.

Следующий этап — независимые компоненты. Мы обучаем ICA на обучающем подмножестве (при необходимости раздельно для read и imagine), автоматически маркируем компоненты с помощью IClabel и исключаем не‑мозговые источники (мигания, ЭМГ и т. п.). В артефакты предобработки сохраняются матрицы смешивания и обратного преобразования, список исключённых компонент и полный журнал решений. Далее на валидации и тесте используется ровно то же преобразование без переобучения. Для повышения качества сигнала поверх этого применяется адаптивная вейвлет‑фильтрация (семейство Добеши, мягкий трешолдинг), причём выбор семейства и уровня разложения фиксируется по обучающему множеству и протоколируется для воспроизводимости.

Нормализация выполняется в одном из двух режимов. В локальном режиме каждая эпоха масштабируется по каналам с использованием собственных средних и стандартных отклонений, что является детерминированной процедурой без обучения на тесте. В глобальном режиме статистики по каналам оцениваются на обучении текущего фолда, сериализуются и затем применяются неизменно к валидационным и тестовым данным. При необходимости используется робастная версия с межквартильным размахом. На всех шагах формируются отчёты контроля качества: доля интерполированных каналов, остаточная сетевая мощность, распределения канал‑специфических средних и дисперсий, число исключённых компонент ICA, параметры вейвлет‑фильтрации. Все артефакты снабжаются хэшами и версиями библиотек.

Наконец, сводное правило контроля утечек: любые операции, параметры которых оцениваются по данным (PREP, AutoReject, ICA, глобальная нормализация, выбор параметров вейвлет‑фильтрации), обучаются исключительно на train внутри фолда и далее применяются как неизменяемые преобразования. Сегментация эпох жёстко ограничена заданными интервалами, что исключает появление информации из будущего относительно центра события.

---

## 5. Сборка датасета и контракты данных

Сборка датасета подчиняется принципу «контрактов данных», позволяющему однозначно восстанавливать и проверять состояние каждого примера. Для каждой эпохи формируется машинно‑читаемый манифест, в котором фиксируются идентификаторы субъекта, сессии и прогона, порядковый номер эпохи, домен (read или imagine), исходная фраза и её категориальная метка, границы окна по времени, частота дискретизации, упорядоченный список каналов и хэш монтажа, а также ссылки на все артефакты предобработки (решения PREP, параметры AutoReject, файлы ICA, состояние нормализатора, параметры вейвлет‑фильтра). Помимо этого, манифест содержит хэш полезной нагрузки (например, SHA‑256 массива признаков), версию кода и временную метку создания, что позволяет воспроизводить эксперименты и проверять целостность выгрузок.

Мы поддерживаем два комплементарных формата хранения. Формат FIF используется как эталонное представление эпох с полным набором метаданных, параллельно для обучения сохраняются компактные контейнеры в PKL, HDF5 или Parquet с тензорами признаков (float32) и ссылками на артефакты. Каталожная структура единообразна для всех субъектов и сессий: отдельные директории отведены под эталонные эпохи, итоговые пакеты для обучения, артефакты предобработки, производные признаки и журналы контроля качества.

Привязка фраз к эпохам осуществляется строго до обучения модели и опирается на статичную карту сопоставления «фраза → грубая категория», снабжённую собственным хэшем. Допускаются два режима соответствия: точное совпадение после нормализации строки и высокопороговое нестрогое совпадение (fuzzy). Использование семантического поиска по эмбеддингам как инструмента «доделки» разметки по умолчанию отключено, поскольку вносит в метки модельные предположения. Если такой режим всё же применяется для исследовательского анализа, каждое соответствие помечается пониженной уверенностью и либо исключается из обучающей выборки, либо анализируется отдельно с оценкой доли шумных меток и их влияния на итоговые метрики.

Для сценариев разбиения на обучающие, валидационные и тестовые множества мы предоставляем явные списки индексов эпох для within‑session, cross‑session и cross‑subject (LOSO), а также связанный набор артефактов, обученных на обучающем подмножестве. Структура таких описаний может храниться в файлах split.yaml, где помимо индексов указываются пути к сериализованным решениям PREP, AutoReject и ICA, состоянию нормализатора и параметрам вейвлет‑фильтра. Загрузка датасета сопровождается автоматической верификацией совместимости: проверяются число и порядок каналов, хэш монтажа, хэш карты меток, режим нормализации и границы временного окна. При несовпадениях применяется строгая политика отказа либо заранее задокументированная процедура адаптации.

Для удобства обучения глубинных моделей дополнительно поддерживается экспорт в формат, совместимый с PyTorch: для каждого домена хранится последовательность тензоров (C, T), список словарей меток (включая текст фразы, идентификатор категории и степень уверенности) и исходные длины до возможного паддинга, а в разделе метаданных фиксируются частота дискретизации, число каналов, хэш монтажа, маска каналов, режим нормализации, хэши состояний и версия кода. Все артефакты и манифесты снабжаются контрольными суммами и версионностью, а к каждому релизу датасета прилагается краткий журнал изменений и проверочные хэши контрольных подвыборок.

---

## 6. Модель и обучение (обзорно)

Архитектура модели следует принципу «локально‑частотное извлечение → временная агрегация → дискретизация представлений → многоцелевой семантический супервайзинг». Входной сегмент ЭЭГ $x \in \mathbb{R}^{C \times T}$ сначала проходит через сверточный стем, который извлекает локально‑частотные и краткосрочные временные признаки и понижает размерность по времени. Далее эти карты подаются на трансформер‑энкодер, моделирующий дальнодействующие зависимости самовниманием. Полученное агрегированное представление проецируется в семантическое пространство размерности $d=768$ и подвергается векторной квантизации: используется кодбук из $M=256$ векторов, обновляемый по схеме EMA с мягкой дискретизацией через Gumbel‑Softmax. В первых $W=5$ эпохах применяется warm‑up (дискретизация отключена), затем включается аннилирование температуры по закону $\tau_t = \max\{\tau_{\min}, \tau_0 \cdot \gamma^t\}$ (типично $\tau_0=0.05$, $\gamma=0.95$, $\tau_{\min}=0.01$). Квантизованное представление $z_q$ служит единым узлом согласования всех целевых задач.

Поверх $z_q$ обучаются вспомогательные головы. Классификатор грубых семантических категорий реализован многослойным перцептроном с dropout и оптимизируется по перекрёстной энтропии. Доменный классификатор (read/imagine) подключается адверсариально через операцию реверса градиента: в прямом ходе он предсказывает домен, а в обратном «штрафует» энкодер за доменно‑специфичные признаки, продвигая их к инвариантности. Реконструктор сигнала восстанавливает исходный $x$ из $z_q$ и выступает как регуляризатор сохранения низкоуровневой информации. Дополнительно используется семантический декодер на базе Transformer, который напрямую аппроксимирует замороженные текстовые эмбеддинги $v_{\text{text}} = g(y)$ из $z_q$. Это устраняет разрыв «кодбук → семантика» и стабилизирует выравнивание пространств. На промежуточных уровнях (после CNN и после Transformer) включён иерархический семантический супервизор, формирующий вспомогательные проекции и ошибки, что эмпирически улучшает обучаемость и структурированность скрытых представлений.

Обучение формулируется как многокритериальная оптимизация. Базовая часть — контрастивная NT‑Xent‑потеря между $z_q$ и $v_{\text{text}}$:

$$
L_{\text{NTX}}(i) = -\log\left( \frac{\exp(\text{sim}(z_i, v_i)/\tau)}{\sum_{j \neq i} \exp(\text{sim}(z_i, v_j)/\tau)} \right),
$$

где $\text{sim}(u,v) = \cos(u,v)$. 

Наряду с «жёсткой» версией используется soft‑вариант, учитывающий семантические близости между фразами: положительные веса распределяются между парафразами и близкими формулировками, что снижает штраф за разумные подстановки. Векторная квантизация оптимизируется стандартным слагаемым с «обязательством» (commitment):

$$
L_{\text{VQ}} = \| \text{sg}[z] - e \|_2^2 + \beta \cdot \| z - \text{sg}[e] \|_2^2
$$

($e$ — ближайший вектор кодбука, $\text{sg}$ — остановка градиента, $\beta \approx 0.25$). 

Для предотвращения коллапса кодбука добавляется регуляризация по энтропии распределения использования кодов (эквивалент поощрению высокой перплексии). Метрическое обучение усиливается триплет‑потерей с semi‑hard negative mining и отступом $m=0.3$, а структуризация семантики — перекрёстной энтропией по coarse‑классам. Доменно‑инвариантное обучение реализовано через адверсарное слагаемое с реверсом градиента, а сохранность информации о сигнале обеспечивается MSE‑реконструкцией. Наконец, прямое семантическое принуждение выполняется через декодер:

$$
L_{\text{sem}} = \alpha \cdot \| D(z_q) - v_{\text{text}} \|_2^2 + (1 - \alpha) \cdot (1 - \cos(D(z_q), v_{\text{text}})).
$$

Итоговая функция потерь имеет вид

$$
L = \lambda_{\text{NTX}} \cdot L_{\text{NTX}} + \lambda_{\text{sem}} \cdot L_{\text{sem}} + \lambda_{\text{trip}} \cdot L_{\text{triplet}} + \lambda_{\text{coarse}} \cdot L_{\text{coarse}} + \lambda_{\text{dom}} \cdot L_{\text{domain}} + \lambda_{\text{rec}} \cdot \| \hat{x} - x \|_2^2 + \lambda_{\text{VQ}} \cdot L_{\text{VQ}} + \lambda_{\text{ent}} \cdot L_{\text{entropy}} + \lambda_{\text{aux}} \cdot L_{\text{aux}},
$$

где весовые коэффициенты подбираются на валидации. Оптимизация выполняется AdamW с клиппингом нормы градиента.

Важной практической деталью является способ работы с текстовыми целями. Текстовый энкодер $g(\cdot)$ заморожен и используется только для получения эталонных эмбеддингов фраз. Для повышения устойчивости к формулировкам и синонимии в обучении и особенно при оценке применяется парафразирование: для каждой фразы генерируется набор близких выражений, их эмбеддинги усредняются, формируя «эталон‑ансамбль». Инференс сводится к косинусному сопоставлению нормированных $z_q$ с нормированными эталонными векторами всего словаря, top‑k кандидаты определяются ранжированием по сходству. Такая схема, совместно с доменно‑адверсарной регуляризацией и иерархической супервизией, обеспечивает устойчивое выравнивание «мозг → текст» и улучшает переносимость между доменами read и imagine.

---

## 7. Инференс и калибровка доверия

На этапе инференса латентный вектор преобразуется в L2-нормированный эмбеддинг и сравнивается с заранее подготовленными, также нормированными эталонными векторами фраз из словаря. В качестве меры близости используется косинусное сходство (эквивалентно скалярному произведению в нормированном пространстве). Эталонные векторы хранятся в индексе приближённого ближайшего соседа (например, FAISS): для точных поисков применяется плоский индекс по скалярному произведению, для ускорения — иерархические (IVF с настраиваемыми параметрами nlist/nprobe) или графовые (HNSW) структуры. Все версии индекса фиксируются и подписываются хэшами, что обеспечивает воспроизводимость.

Выдача формируется как упорядоченный набор top‑k кандидатов с их исходными оценками сходства. Пост‑обработка включает дедупликацию парафраз (если словарь содержит варианты одной и той же фразы), правила разрешения «ничьих» (например, предпочтение более частых или более коротких формулировок), а также опциональный доменный приор: когда доменная голова (read/imagine) даёт высокую уверенность, ранжирование может проводиться в под‑индексе соответствующего домена. Для устойчивости к формулировкам применяется ансамблирование эталонов: каждой фразе соответствует набор парафраз, эмбеддинги которых усредняются, что уменьшает чувствительность к синонимии и пунктуации.

Калибровка доверия проводится отдельно для retrieval и для классификаций (coarse и домен). Для retrieval оценки сходства трактуются как пред‑логиты и пропускаются через температурное масштабирование: вероятности получаются softmax от логитов, делённых на положительную температуру. Параметр температуры подбирается только на валидационном множестве, оптимизируя отрицательное правдоподобие или Brier‑потерю. Альтернативой служит изотоническая регрессия (монотонная, непараметрическая калибровка). Для классовых задач при выраженной несбалансированности допустимо класс‑специфическое температурное масштабирование. Качество калибровки оценивается по ожидаемой калибровочной ошибке (ECE) с разбиением интервала уверенности на бины, дополнительно приводятся Brier‑скор и «reliability‑кривые» (accuracy vs. confidence) до и после калибровки. Доверительные интервалы для ECE и Brier оцениваются бутстрэпом по эпизодам (95% CI).

Для отсечения малонадежных ответов и маршрутизации «на человека» вычисляются стандартные меры неопределённости: максимум предсказанной вероятности, энтропия распределения, разница между первым и вторым кандидатом (margin), а также плотность поддержки среди ближайших соседей (сколько кандидатов превосходят порог сходства). На их основе реализуется стратегия «отказа» (abstain) с порогами, настроенными по кривым риск‑покрытие. В расширенном режиме поддерживаются конформные предсказательные множества: на валидации оценивается шкала невключения, а на тесте возвращается минимальный набор фраз с заданным гарантированным покрытием (1−α) при контролируемой ширине.

С инженерной стороны инференс реализован батчево, поддерживает смешанную точность и кэширование эмбеддингов для повторяющихся окон, а также «быстрый проход» с малым k и агрессивными параметрами индекса для предварительного ранжирования с последующим доранжированием в точном режиме. Параметры калибровки и версии индексов строго версионируются, процедуры подгонки калибровки никогда не используют тестовые данные, что зафиксировано в журналах экспериментов и обеспечивает строгую воспроизводимость.

---

## 8. Протокол оценки и метрики

**Сплиты:** within‑session / cross‑session / cross‑subject (LOSO). Все предобработчики, скейлеры, ICA, пороги — fit‑on‑train per‑fold.
**Метрики (retrieval):** Top‑1/5/10, MRR, Recall\@k, nDCG\@k (опционально) + 95% CI (бутстрэп по триалам).
**Метрики (классификация):** accuracy, macro‑F1, balanced accuracy; матрицы ошибок.
**Калибровка:** ECE/ACE, Brier.
**Статистика:** пермутационный тест против случайного ранжирования и между моделями/абляциями.

**Бейзлайны:**

* Random;
* частотно‑полосные признаки + линейная модель/логрег;
* EEGNet / ShallowFBCSPNet;
* наш без‑VQ вариант;
* наш полный вариант.

**Абляции:** −VQ, −Recon, −AuxHeads, −Contrastive.

---

## 9. Результаты (предварительно, обзорно)

В v1 приводим качественные иллюстрации (t‑SNE/UMAP $z_q$ vs текстовые эмбеддинги, распределения кодбука), примеры top‑k выдачи, confusion‑матрицы по coarse/домену и reliability‑кривые. Численные таблицы по всем метрикам и сплитам будут расширены в v2 вместе с CI и сравнениями бейзлайнов/абляций.

---

## 10. Воспроизводимость и пакет артефактов

* **Окружение:** `conda env.yml` или `Dockerfile`, фиксация версий/seed, `requirements.txt`.
* **Конфигурации:** Hydra‑конфиги (`configs/`), `Makefile`/`invoke` для сценариев.
* **Данные:** скрипты загрузки/валидации, манифест и хэши словаря фраз/маппинга.
* **Обучение/оценка:** `train.py`, `eval.py`, `ablation.py`, `baseline.py`.
* **Индексация:** скрипт сборки индекса кандидатов; фиксация версий текстового энкодера.
* **Логи/артефакты:** DVC (`dvc.yaml`) или эквивалент, чекпоинты, конфиги, отчёты.
* **MODEL\_CARD.md:** назначение, ограничения, риски, области применения.

---

## 11. Ограничения

* Небольшое количество субъектов и вариативность межсубъектного переноса.
* Возможный шум в разметке фраз, отсутствие OOD‑оценки.
* Отсутствие онлайн‑адаптации и real‑time экспериментов в v1.

---

## 12. Практическая ценность и применения

Воспроизводимый каркас полезен для исследований внутренней речи, прототипирования BCI‑интерфейсов, а также как компонент мультимодальных систем «мозг↔компьютер». Единые контракты данных и стандартизованные шаги обработки упрощают сравнение моделей и накопление результатов.

---

## 13. Заключение и планы

Мы представили системный конвейер «EEG→Text» с акцентом на воспроизводимость и прозрачную оценку. В ближайших версиях: 

1. Полные таблицы результатов со статистикой и CI; 
2. Cильные бейзлайны и абляции; 
3. Перенос между сессиями и субъектами; 
4. Расширенная калибровка доверия и VQ‑диагностика.

---

## Благодарности

*Здесь — благодарности коллегам, лаборатории и участникам сбора данных.*

## Ссылки

- [Lee et al., 2020] Neural Decoding of Imagined Speech and Visual Imagery as Intuitive Paradigms for BCI Communication. IEEE TNSRE 28(12):2647–2659.
- [Abdulghani et al., 2023] Imagined Speech Classification Using EEG and Deep Learning. MDPI Bioengineering 10(6):649.
- [Alharbi & Alotaibi, 2024] Decoding Imagined Speech from EEG Data: A Hybrid Deep Learning Approach to Capturing Spatial and Temporal Features. Life 14(11):1501.
- [Amrani et al., 2024] Deep Representation Learning for Open Vocabulary EEG‑to‑Text Decoding. IEEE JBHI / arXiv:2312.09430.
- [El Gedawy et al., 2025] Bridging Brain Signals and Language: A Deep Learning Approach to EEG‑to‑Text Decoding. arXiv:2502.17465.
- [Jiang et al., 2025] Neural Spelling: A Spell‑Based BCI System for Language Neural Decoding. arXiv:2501.17489.

---

### Приложение A. Anti‑leakage чек‑лист

* Fit всех предобработчиков/скейлеров/ICA/вейвлет‑порогов — **только на train per‑fold**.
* Гиперпараметры подбираются на **validation**, не на test.
* Текстовый энкодер для индекса/разметки **не обучается** на ваших данных и не совпадает с используемым в модели; при совпадении — удалить семантический fallback.
* Индекс и словарь фраз фиксируются до обучения (хэшируются).
* Проверка time‑leak: строгое временное окно инференса, без доступа к пост‑событийным сигналам.

### Приложение B. Диагностика VQ

* Перплексия и «живость» кодов, частотные распределения.
* Влияние размера кодбука и $\beta$ на качество и интерпретацию.
* Примеры паттернов кодов, связанных с coarse/доменом.

### Приложение C. Структура «репро‑пакета» (пример)

```
repo/
  ├─ env.yml | Dockerfile
  ├─ configs/
  ├─ data/
  │   └─ manifests/, hashes/
  ├─ src/
  │   ├─ preprocess/
  │   ├─ dataset/
  │   ├─ models/
  │   ├─ train.py  eval.py  ablation.py  baseline.py
  ├─ index/
  ├─ artifacts/  logs/
  ├─ MODEL_CARD.md  README.md  LICENSE
```

### Приложение D. Карточка модели (MODEL\_CARD.md, краткий шаблон)

**Назначение.** Retrieval по словарю фраз и классификация coarse/домена из ЭЭГ.
**Данные.** Источник, протокол сборки, согласия.
**Обучение.** Архитектура, лоссы, гиперпараметры.
**Оценка.** Сплиты, метрики, CI.
**Ограничения.** Области, где модель ненадёжна.
**Этика и риск.** Конфиденциальность, допустимые применения.
**Контакты.** Поддержка и связь.

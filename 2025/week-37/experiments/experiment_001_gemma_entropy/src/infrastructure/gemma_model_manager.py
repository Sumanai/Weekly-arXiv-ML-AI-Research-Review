"""
–ú–æ–¥—É–ª—å –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –º–æ–¥–µ–ª–∏ Gemma-3n-E2B-it (–∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä–∞).

–í—ã –¥–æ–ª–∂–Ω—ã —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –≤–µ—Å—å –∫–ª–∞—Å—Å GemmaModelManager.

–ü—Ä–µ–¥–ª–∞–≥–∞—é –≤–∞–º –¥—Ä–æ–ø–Ω—É—Ç—å –≤–µ—Å—å –∫–æ–¥, –∏ –æ—Å—Ç–∞–≤–∏—Ç—å —Ç–æ–ª—å–∫–æ –ø–æ–¥—Å–∫–∞–∑–∫–∏ TODO, —á—Ç–æ –Ω—É–∂–Ω–æ —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å ü§ó
"""

# –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –±–∏–±–ª–∏–æ—Ç–µ–∫–∞
import logging
from pathlib import Path
from typing import Any, Dict, Optional

# –°—Ç–æ—Ä–æ–Ω–Ω–∏–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏
import torch
import yaml
from transformers import AutoModelForCausalLM, AutoTokenizer

from domain.ports import ModelPort

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class GemmaModelManager(ModelPort):
    """
    Description:
    ---------------
        –ö–ª–∞—Å—Å-–æ–±—ë—Ä—Ç–∫–∞ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏, –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞,
        —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –∏ –º–æ–¥–µ–ª–∏ Gemma-3n-E2B-it.

    Examples:
    ---------------
        >>> manager = GemmaModelManager("config/experiment.yaml")
        >>> manager.load_model()
        >>> info = manager.get_model_info()
        >>> isinstance(info, dict)
        True
    """

    def __init__(self, config_path: str = "config/experiment.yaml") -> None:
        """
        Description:
        ---------------
            –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –º–µ–Ω–µ–¥–∂–µ—Ä –º–æ–¥–µ–ª–∏: –∑–∞–≥—Ä—É–∂–∞–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é,
            –ø–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –∞—Ç—Ä–∏–±—É—Ç—ã –º–æ–¥–µ–ª–∏, —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –∏ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞.

        Args:
        ---------------
            config_path: –ø—É—Ç—å –∫ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω–æ–º—É —Ñ–∞–π–ª—É.

        Returns:
        ---------------
            None

        Raises:
        ---------------
            FileNotFoundError: –µ—Å–ª–∏ —Ñ–∞–π–ª –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω

        Examples:
        ---------------
            >>> manager = GemmaModelManager("config/experiment.yaml")
            >>> manager.model is None
            True
        """
        # TODO: –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–π—Ç–µ —Å–ª–µ–¥—É—é—â–∏–µ –∞—Ç—Ä–∏–±—É—Ç—ã:
        # - self.config (–∑–∞–≥—Ä—É–∑–∏—Ç–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é —á–µ—Ä–µ–∑ _load_config)
        # - self.model = None
        # - self.tokenizer = None
        # - self._device (–æ–ø—Ä–µ–¥–µ–ª–∏—Ç–µ CUDA –∏–ª–∏ CPU)
        # –í—ã–≤–µ–¥–∏—Ç–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ —á–µ—Ä–µ–∑ logger.info

        # pass

        self.config = self._load_config(config_path)
        self.model = None
        self.tokenizer = None
        self._device = torch.device("cpu")
        logger.info("Device: %s", self._device)

    def _load_config(self, config_path: str) -> Dict[str, Any]:
        """
        Description:
        ---------------
            –ó–∞–≥—Ä—É–∂–∞–µ—Ç YAML-–∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –µ—ë
            –∫–∞–∫ —Å–ª–æ–≤–∞—Ä—å.

        Args:
        ---------------
            config_path: –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏.

        Returns:
        ---------------
            dict: –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è.

        Raises:
        ---------------
            FileNotFoundError: –µ—Å–ª–∏ —Ñ–∞–π–ª –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω

        Examples:
        ---------------
            >>> manager = GemmaModelManager()
            >>> isinstance(manager.config, dict)
            True
        """
        # TODO: —Ä–µ–∞–ª–∏–∑—É–π—Ç–µ –∑–∞–≥—Ä—É–∑–∫—É YAML –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏:
        # 1. –°–æ–∑–¥–∞–π—Ç–µ Path –æ–±—ä–µ–∫—Ç –∏–∑ config_path
        # 2. –ï—Å–ª–∏ —Ñ–∞–π–ª –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π –ø—É—Ç—å –æ—Ç —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
        # 3. –û—Ç–∫—Ä–æ–π—Ç–µ –∏ –∑–∞–≥—Ä—É–∑–∏—Ç–µ YAML —Ñ–∞–π–ª —Å –ø–æ–º–æ—â—å—é yaml.safe_load
        # 4. –í–µ—Ä–Ω–∏—Ç–µ —Å–ª–æ–≤–∞—Ä—å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏

        # pass

        config_path = Path(config_path)
        if not config_path.exists():
            config_path = Path(__file__).parent.parent / config_path
        with open(config_path, "r") as f:
            config = yaml.safe_load(f)
        return config

    def load_model(self) -> None:
        """
        Description:
        ---------------
            –ó–∞–≥—Ä—É–∂–∞–µ—Ç —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∏ –º–æ–¥–µ–ª—å –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏, –ø—Ä–∏–≤–æ–¥–∏—Ç
            –º–æ–¥–µ–ª—å –≤ —Ä–µ–∂–∏–º eval –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å—Å—ã–ª–∫–∏ –≤ –∞—Ç—Ä–∏–±—É—Ç—ã `self.model`
            –∏ `self.tokenizer`.

        Args:
        ---------------
            –ù–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è `self.config`.

        Returns:
        ---------------
            None

        Raises:
        ---------------
            KeyError: –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –∫–ª—é—á–∏ –≤ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
            RuntimeError: –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –∏–ª–∏ –º–æ–¥–µ–ª–∏

        Examples:
        ---------------
            >>> manager = GemmaModelManager("config/experiment.yaml")
            >>> manager.load_model()
            >>> manager.model is not None and manager.tokenizer is not None
            True
        """
        # TODO: —Ä–µ–∞–ª–∏–∑—É–π—Ç–µ –∑–∞–≥—Ä—É–∑–∫—É –º–æ–¥–µ–ª–∏:
        # 1. –ü–æ–ª—É—á–∏—Ç–µ –∏–º—è –º–æ–¥–µ–ª–∏ –∏–∑ self.config['model']['name']
        # 2. –ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä —á–µ—Ä–µ–∑ AutoTokenizer.from_pretrained
        #    —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–º trust_remote_code=True
        # 3. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ pad_token = eos_token –µ—Å–ª–∏ pad_token –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç
        # 4. –ó–∞–≥—Ä—É–∑–∏—Ç–µ –º–æ–¥–µ–ª—å —á–µ—Ä–µ–∑ AutoModelForCausalLM.from_pretrained
        #    —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏: torch_dtype=torch.bfloat16, device_map –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞, trust_remote_code=True
        # 5. –ü–µ—Ä–µ–≤–µ–¥–∏—Ç–µ –º–æ–¥–µ–ª—å –≤ —Ä–µ–∂–∏–º eval()
        # 6. –í—ã–≤–µ–¥–∏—Ç–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∑–∞–≥—Ä—É–∑–∫–µ –∏ —Ä–∞–∑–º–µ—Ä–µ —Å–ª–æ–≤–∞—Ä—è

        # pass

        try:
            model_name = self.config["model"]["name"]
        except KeyError as error:
            raise KeyError("–í –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –∫–ª—é—á 'model.name'") from error

        try:
            tokenizer = AutoTokenizer.from_pretrained(
                model_name,
                trust_remote_code=True,
            )
            if tokenizer.pad_token is None:
                tokenizer.pad_token = tokenizer.eos_token

            model = AutoModelForCausalLM.from_pretrained(
                pretrained_model_name_or_path=model_name,
                torch_dtype=torch.float32,
                device_map="cpu",
                trust_remote_code=True,
            )
        except Exception as error:
            raise RuntimeError("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å –∏–ª–∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä") from error

        model.eval()

        logger.info("Model loaded successfully")
        logger.info("Vocab size: %d", len(tokenizer))
        logger.info("Model type: %s", type(model).__name__)
        logger.info(
            "Model parameters: %d",
            sum(p.numel() for p in model.parameters()),
        )
        logger.info(
            "Trainable parameters: %d",
            sum(p.numel() for p in model.parameters() if p.requires_grad),
        )

        self.model = model
        self.tokenizer = tokenizer

    # –†–µ–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ—Ä—Ç–æ–≤—ã—Ö –º–µ—Ç–æ–¥–æ–≤
    def tokenize(self, text: str, *, max_length: int) -> Dict[str, Any]:
        inputs = self.tokenizer(
            text,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=max_length,
        )
        return {k: v.to(self._device) for k, v in inputs.items()}

    def forward(self, **inputs: Any) -> Any:
        with torch.no_grad():
            return self.model(**inputs)

    def convert_ids_to_tokens(self, ids_tensor: Any) -> Any:
        return self.tokenizer.convert_ids_to_tokens(ids_tensor)

    def decode_token(self, token_id_tensor: Any) -> str:
        return self.tokenizer.decode(token_id_tensor[0], skip_special_tokens=True)

    def decode_sequence(self, token_ids: Any) -> str:
        return self.tokenizer.decode(token_ids, skip_special_tokens=True)

    def eos_token_id(self) -> int:
        return int(self.tokenizer.eos_token_id)

    def context_length(self) -> int:
        return int(self.config["model"]["context_length"])

    @property
    def device(self) -> Any:
        return self._device

    def get_model_info(self) -> Optional[Dict[str, Any]]:
        """
        Description:
        ---------------
            –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–≤–æ–¥–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –∏ –æ–∫—Ä—É–∂–µ–Ω–∏–∏.

        Returns:
        ---------------
            dict | None: –°–ª–æ–≤–∞—Ä—å —Å –ø–æ–ª—è–º–∏ –æ –º–æ–¥–µ–ª–∏ –∏–ª–∏ None, –µ—Å–ª–∏ –º–æ–¥–µ–ª—å
            –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞.

        Examples:
        ---------------
            >>> manager = GemmaModelManager()
            >>> manager.load_model()
            >>> info = manager.get_model_info()
            >>> isinstance(info, dict)
            True
        """
        # TODO: –†–µ–∞–ª–∏–∑—É–π—Ç–µ –ø–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –º–æ–¥–µ–ª–∏:
        # –ï—Å–ª–∏ –º–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞, –≤–µ—Ä–Ω–∏—Ç–µ None
        # –ò–Ω–∞—á–µ –≤–µ—Ä–Ω–∏—Ç–µ —Å–ª–æ–≤–∞—Ä—å —Å –∫–ª—é—á–∞–º–∏:
        # - model_name: –∏–º—è –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        # - vocab_size: —Ä–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
        # - device: —Å—Ç—Ä–æ–∫–æ–≤–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞
        # - model_type: —Ç–∏–ø –º–æ–¥–µ–ª–∏ —á–µ—Ä–µ–∑ type(self.model).__name__
        # - parameters: –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        # - trainable_parameters: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—É—á–∞–µ–º—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤

        # pass

        if self.model is None:
            return None
        return {
            "model_name": self.config["model"]["name"],
            "vocab_size": len(self.tokenizer),
            "device": str(self._device),
            "model_type": type(self.model).__name__,
            "parameters": sum(p.numel() for p in self.model.parameters()),
            "trainable_parameters": sum(
                p.numel() for p in self.model.parameters() if p.requires_grad
            ),
        }

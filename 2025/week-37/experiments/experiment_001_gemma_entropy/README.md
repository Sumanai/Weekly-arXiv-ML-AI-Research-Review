# Эксперимент 001: Анализ энтропии токенов модели Gemma-3n-E2B-it

## Цель эксперимента
Исследование неопределенности модели Google Gemma-3n-E2B-it через анализ энтропии токенов по формуле:

$H_i = -\sum_{j} P_i(j) \log P_i(j)$

где:
- **$H_i$** — энтропия распределения вероятностей токенов на позиции $i$
- **$P_i(j)$** — вероятность $j$-го токена из словаря в позиции $i$

## Описание модели
- **Модель**: Google Gemma-3n-E2B-it
- **Размер**: 6B параметров (эффективно работает как 2B модель)
- **Возможности**: Мультимодальная (текст, изображения, аудио, видео)
- **Контекст**: До 32K токенов

## Гипотеза
Энтропия токенов будет варьироваться в зависимости от:
1. Сложности контекста (простые vs сложные предложения)
2. Предсказуемости продолжения (устойчивые фразы vs творческие тексты)
3. Позиции в последовательности (начало vs середина vs конец)

## Структура эксперимента
```
experiment_001_gemma_entropy/
├── config/          # Конфигурационные файлы
├── src/             # Исходный код
└── README.md        # Этот файл
```

## Методология
1. Загрузка предобученной модели Gemma-3n-E2B-it
2. Подготовка набора тестовых промптов различной сложности
3. Получение распределений вероятностей для каждого токена
4. Вычисление энтропии по формуле
5. Анализ закономерностей и интерпретация результатов

## Ожидаемые результаты
- Количественная оценка уверенности модели для разных типов текста
- Выявление паттернов в распределении энтропии
- Понимание поведения модели на различных типах задач

## Технические требования
- Python 3.8+
- PyTorch
- HuggingFace Transformers
- CUDA-совместимая GPU (рекомендуется 12GB+ VRAM)
- NumPy, matplotlib для анализа и визуализации
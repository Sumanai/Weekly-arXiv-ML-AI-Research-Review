# Математические основы DeepConf: улучшение рассуждений через оценку уверенности

## Концептуальная основа DeepConf

Deep Think with Confidence (DeepConf) представляет собой простой, но эффективный метод, **устраняющий необходимость в повторной генерации полных цепочек рассуждений** через элегантное использование внутренних сигналов уверенности модели. Метод существенно повышает как эффективность рассуждений, так и вычислительную производительность больших языковых моделей в тестовом режиме.

Принципиальное отличие DeepConf от классических подходов к параллельному мышлению (parallel thinking) заключается в его способности **динамически фильтровать низкокачественные трассы рассуждений** как во время (онлайн), так и после (офлайн) генерации, без необходимости в дополнительном обучении модели или настройке гиперпараметров.

## Метрики уверенности в качестве сигнала качества рассуждений

![Рисунок 5: DeepConf с параллельным мышлением отклоняет трассировки рассуждений с низкой уверенностью во время генерации, чтобы достичь более высокой производительности рассуждений, используя при этом значительно меньше сгенерированных токенов.](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-37/assets/Image-05.png)

![Рисунок 6: Измерение уверенности и уверенное мышление в офлайн-режиме.](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-37/assets/Image-06.png)

Основу DeepConf составляет использование различных метрик уверенности, извлекаемых из распределения следующих токенов модели. Формально, можно выделить следующие ключевые метрики:

### 1. Энтропия токена

$$H_i = -\sum_{j} P_i(j) \log P_i(j)$$

<details> 
    <summary><em><strong>пояснение переменных</strong></em></summary>

где:
- **$H_i$** — энтропия распределения вероятностей токенов на позиции $i$.
- **$P_i(j)$** — вероятность $j$-го токена из словаря в позиции $i$.
</details>

---

Энтропия распределения $P$ — это мера его **неопределённости**. Чем ниже энтропия, тем более "сконцентрировано" распределение вероятностей и тем увереннее модель в своём предсказании. Высокая энтропия означает, что вероятностная масса "размазана" по многим токенам, что указывает на неуверенность модели.

Например, рассмотрим предсказание следующего токена в контексте:
- **Контекст:** `"Теорема Пифагора гласит, что в прямоугольном треугольнике сумма квадратов ___"`
- **Возможные токены:** `"катетов"` (0.9), `"гипотенузы"` (0.05), `"сторон"` (0.05)

Энтропия будет низкой ($H_i \approx 0.47$), указывая на высокую уверенность модели. Если же распределение близко к равномерному, например, `"катетов"` (0.4), `"гипотенузы"` (0.3), `"сторон"` (0.3), энтропия будет высокой ($H_i \approx 1.57$), что говорит о низкой уверенности модели.
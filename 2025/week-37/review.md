# Deep Think with Confidence

## Предпосылки и мотивация

Большие языковые модели (БЯМ) продемонстрировали выдающиеся возможности в сложных задачах рассуждения, но достижение высокой точности часто требует генерации сотен или тысяч цепочек рассуждений с помощью таких методов, как самосогласованность с голосованием по большинству. Хотя этот подход "параллельного мышления" эффективен, он страдает от значительных вычислительных затрат и убывающей отдачи – иногда требуя 100 миллионов дополнительных токенов для скромного улучшения точности на 14% в сложных задачах, таких как AIME 2025.

![Рисунок 1: Сравнение точности на AIME 2025, показывающее превосходную производительность DeepConf для различных размеров моделей, причем некоторые достигают почти идеальной точности (99,9% для GPT-OSS-120B).](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-37/assets/Image-01.png)

Основная проблема заключается в том, что все цепочки рассуждений рассматриваются одинаково при голосовании по большинству, несмотря на то, что БЯМ естественным образом производят цепочки разного качества. Предыдущие подходы пытались использовать глобальные меры уверенности, рассчитанные после полной генерации цепочки, но эти методы не могут уловить локальные колебания рассуждений или обеспечить раннее прекращение низкокачественных путей.

## Основная методология

Deep Think with Confidence (DeepConf) решает эти ограничения с помощью сложной системы измерения уверенности, которая работает на нескольких уровнях детализации:

Локальные метрики уверенности: вместо того чтобы полагаться на глобальные средние значения, DeepConf вводит несколько целевых мер уверенности:

- **Групповая уверенность**: рассчитывается по скользящим окнам токенов (обычно 1024-2048 токенов) для сглаживания индивидуальных колебаний токенов.
- **Групповая уверенность нижних 10%**: фокусируется на наиболее проблемных сегментах в цепочке.
- **Минимальная групповая уверенность**: идентифицирует единственный наименее уверенный шаг рассуждения.
- **Уверенность в хвосте**: оценивает надежность финальных шагов рассуждения.

Математическая основа использует уверенность токена, определяемую как:

$$C_i = -\frac{1}{k}\sum_{j=1}^{k} \log P_{\theta}(t_j^{(i)} | x, t_{<i})$$

где $P_{\theta}$ представляет предсказанную моделью вероятность для $j$-го наиболее вероятного токена в позиции $i$.

**Два режима работы**: DeepConf работает как в автономной, так и в онлайн-конфигурациях:

**Автономный режим**: использует взвешенное по уверенности голосование по большинству с фильтрацией, сохраняя только верхние η процентов цепочек на основе оценок уверенности до агрегации.

**Онлайн-режим**: реализует раннюю остановку в реальном времени с использованием динамически калиброванных порогов. Система генерирует начальный набор цепочек для "разогрева", чтобы установить пороги остановки, затем прерывает новые цепочки, чья групповая уверенность падает ниже этого порога.

## Ключевые технические инновации

Эффективность метода обусловлена несколькими техническими инновациями:

**Динамическая калибровка порога:** для онлайн-генерации DeepConf использует начальную фазу "разогрева" с небольшим набором трасс (обычно 16) для установления порогов уверенности, специфичных для проблемы. Этот адаптивный подход гарантирует, что критерии остановки калибруются в соответствии со сложностью каждой проблемы.

**Взвешенная по уверенности агрегация:** вместо простого голосования по большинству DeepConf взвешивает каждый ответ по его связанной уверенности трассы, отдавая приоритет ответам из более надежных путей рассуждений.

**Адаптивное обнаружение консенсуса:** система отслеживает силу консенсуса во время генерации, останавливаясь при достижении достаточного согласия среди высокоуверенных трасс, что дополнительно снижает вычислительные требования.

# ü§ñ Qwen3 0.6B MoE - Mixture-of-Experts Transformer

> –ü–æ–ª–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã Mixture-of-Experts Transformer —Å –Ω—É–ª—è –Ω–∞ PyTorch

[![Python](https://img.shields.io/badge/Python-3.9+-blue.svg)](https://www.python.org/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)](https://pytorch.org/)
[![Tests](https://img.shields.io/badge/Tests-70%2B%20passing-success.svg)]()
[![Progress](https://img.shields.io/badge/Progress-60%25-yellow.svg)]()

---

## üìã –û–ø–∏—Å–∞–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞

–î–∞–Ω–Ω—ã–π –ø—Ä–æ–µ–∫—Ç –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π **–æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—É—é —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—é** –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã Mixture-of-Experts (MoE) Transformer, –æ—Å–Ω–æ–≤–∞–Ω–Ω—É—é –Ω–∞ –º–æ–¥–µ–ª–∏ Qwen3, –Ω–æ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω—É—é –¥–ª—è –º–µ–Ω—å—à–µ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ (0.6B –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤).

### –ö–ª—é—á–µ–≤—ã–µ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏:

- ‚úÖ **–ü–æ–ª–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è —Å –Ω—É–ª—è** - –≤—Å–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –Ω–∞–ø–∏—Å–∞–Ω—ã –≤—Ä—É—á–Ω—É—é
- ‚úÖ **–î–µ—Ç–∞–ª—å–Ω–æ –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–æ** - 70+ unit —Ç–µ—Å—Ç–æ–≤ (–≤—Å–µ –ø—Ä–æ—Ö–æ–¥—è—Ç)
- ‚úÖ **–ü–æ–¥—Ä–æ–±–Ω–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è** - –∫–∞–∂–¥—ã–π –∫–æ–º–ø–æ–Ω–µ–Ω—Ç —Å –ø—Ä–∏–º–µ—Ä–∞–º–∏
- ‚úÖ **–£—á–µ–±–Ω–∞—è –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ—Å—Ç—å** - TODO-—à–∞–±–ª–æ–Ω—ã –∏ –≤–æ–ø—Ä–æ—Å—ã –¥–ª—è —Ä–∞–∑–º—ã—à–ª–µ–Ω–∏—è
- ‚úÖ **DDD –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞** - —á–∏—Å—Ç–∞—è –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è –∫–æ–¥–∞

---

## üéØ –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –º–æ–¥–µ–ª–∏

### –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è Qwen3 0.6B MoE

```
Model Size:        0.6B –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
Num Experts:       8 (–≤–º–µ—Å—Ç–æ 128 –≤ Qwen3-30B)
Active Experts:    2 per token (–≤–º–µ—Å—Ç–æ 8 –≤ Qwen3-30B)
Activation:        25% —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ (vs 6.25% –≤ 30B)
Hidden Size:       512
Intermediate Size: 2048 (4 √ó hidden_size)
Attention:         Grouped-Query Attention (GQA)
Normalization:     RMSNorm
Position Encoding: RoPE (Rotary Position Embedding)
Activation:        SwiGLU
```

### –û—Å–Ω–æ–≤–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã

```
Qwen3 MoE Model
‚îÇ
‚îú‚îÄ‚îÄ Embedding Layer
‚îÇ
‚îú‚îÄ‚îÄ N √ó MoE Transformer Blocks
‚îÇ   ‚îú‚îÄ‚îÄ RMSNorm
‚îÇ   ‚îú‚îÄ‚îÄ Grouped-Query Attention (GQA)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Query projection (8 groups)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Key/Value projection (shared)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ RoPE position encoding
‚îÇ   ‚îú‚îÄ‚îÄ RMSNorm
‚îÇ   ‚îî‚îÄ‚îÄ MoE Feed-Forward Layer
‚îÇ       ‚îú‚îÄ‚îÄ MoE Router (Top-K gating)
‚îÇ       ‚îú‚îÄ‚îÄ 8 √ó Expert Networks (SwiGLU)
‚îÇ       ‚îî‚îÄ‚îÄ Load Balancing Loss
‚îÇ
‚îî‚îÄ‚îÄ LM Head (Output projection)
```

---

## üìä –ü—Ä–æ–≥—Ä–µ—Å—Å —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏

### ‚úÖ –§–∞–∑–∞ 1: –ë–∞–∑–æ–≤–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ (100%)

| –ö–æ–º–ø–æ–Ω–µ–Ω—Ç | –°—Ç–∞—Ç—É—Å | –¢–µ—Å—Ç—ã | –§–∞–π–ª |
|-----------|--------|-------|------|
| RMSNorm | ‚úÖ –ó–∞–≤–µ—Ä—à–µ–Ω–æ | ‚úÖ –í—Å–µ | `experiments/domain/normalization/rmsnorm.py` |
| RoPE | ‚úÖ –ó–∞–≤–µ—Ä—à–µ–Ω–æ | ‚úÖ –í—Å–µ | `experiments/domain/positional_encoding/rope.py` |
| SwiGLU | ‚úÖ –ó–∞–≤–µ—Ä—à–µ–Ω–æ | ‚úÖ –í—Å–µ | `experiments/domain/activations/swiglu.py` |
| GQA | ‚úÖ –ó–∞–≤–µ—Ä—à–µ–Ω–æ | ‚úÖ 12/12 | `experiments/domain/attention/gqa.py` |
| TransformerBlock | ‚úÖ –ó–∞–≤–µ—Ä—à–µ–Ω–æ | ‚úÖ 14/14 | `experiments/domain/transformer/transformer_block.py` |

### üü° –§–∞–∑–∞ 2: MoE –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã (85%)

| –ö–æ–º–ø–æ–Ω–µ–Ω—Ç | –°—Ç–∞—Ç—É—Å | –¢–µ—Å—Ç—ã | –§–∞–π–ª |
|-----------|--------|-------|------|
| MoE Router | ‚úÖ –ó–∞–≤–µ—Ä—à–µ–Ω–æ | ‚úÖ 15/15 | `experiments/domain/moe/router.py` |
| Expert Network | ‚úÖ –ó–∞–≤–µ—Ä—à–µ–Ω–æ | ‚úÖ 15/15 | `experiments/domain/moe/expert.py` |
| MoE Layer | üîÑ –í –ø—Ä–æ—Ü–µ—Å—Å–µ | ‚è≥ 0/12 | `experiments/domain/moe/moe_layer.py` |
| MoE TransformerBlock | ‚è≥ –û–∂–∏–¥–∞–Ω–∏–µ | - | - |

### ‚è≥ –§–∞–∑–∞ 3: –ü–æ–ª–Ω–∞—è –º–æ–¥–µ–ª—å (0%)

- ‚è≥ –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ 0.6B
- ‚è≥ Embedding —Å–ª–æ–π
- ‚è≥ –°–±–æ—Ä–∫–∞ N √ó MoE Transformer Blocks
- ‚è≥ LM Head
- ‚è≥ Utilities –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏

---

## üöÄ –ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç

### –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π

```bash
pip install torch pytest
```

### –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–æ–≤

```bash
# –í—Å–µ —Ç–µ—Å—Ç—ã
pytest experiments/ -v

# –û—Ç–¥–µ–ª—å–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
pytest experiments/domain/moe/test/test_router.py -v
pytest experiments/domain/moe/test/test_expert.py -v
pytest experiments/domain/attention/test/test_gqa.py -v
```

### –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ MoE

```bash
python3 experiments/domain/moe/test_integration.py
```

### –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤

```python
import torch
from experiments.domain.moe.router import MoERouter
from experiments.domain.moe.expert import Expert

# –°–æ–∑–¥–∞–Ω–∏–µ Router –¥–ª—è –º–æ–¥–µ–ª–∏ 0.6B
router = MoERouter(
    hidden_size=512,
    num_experts=8,
    top_k=2
)

# –°–æ–∑–¥–∞–Ω–∏–µ Expert Network
expert = Expert(
    hidden_size=512,
    intermediate_size=2048
)

# Forward pass
x = torch.randn(2, 10, 512)  # (batch, seq, hidden)
routing_weights, selected_experts, balance_loss = router(x)
```

---

## üìÅ –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø—Ä–æ–µ–∫—Ç–∞

```
experiments/
‚îú‚îÄ‚îÄ domain/
‚îÇ   ‚îú‚îÄ‚îÄ normalization/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ rmsnorm.py              # RMS Normalization
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ test/
‚îÇ   ‚îú‚îÄ‚îÄ positional_encoding/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ rope.py                 # Rotary Position Embedding
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ test/
‚îÇ   ‚îú‚îÄ‚îÄ activations/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ swiglu.py              # SwiGLU Activation
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ SwiGLU.md              # –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ test/
‚îÇ   ‚îú‚îÄ‚îÄ attention/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ gqa.py                 # Grouped-Query Attention
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ GQA.md                 # –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ GQA_Forward_Explained.md
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ test/
‚îÇ   ‚îú‚îÄ‚îÄ transformer/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ transformer_block.py   # Transformer Block
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ test/
‚îÇ   ‚îî‚îÄ‚îÄ moe/
‚îÇ       ‚îú‚îÄ‚îÄ router.py              # MoE Router (Top-K gating)
‚îÇ       ‚îú‚îÄ‚îÄ expert.py              # Expert Network
‚îÇ       ‚îú‚îÄ‚îÄ moe_layer.py           # MoE Layer (–∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è)
‚îÇ       ‚îú‚îÄ‚îÄ MoE_Router_Gate_Initialization.md     # –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è (885 —Å—Ç—Ä–æ–∫)
‚îÇ       ‚îú‚îÄ‚îÄ MoE_Router_Load_Balancing_Loss.md     # –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è (1067 —Å—Ç—Ä–æ–∫)
‚îÇ       ‚îú‚îÄ‚îÄ test_integration.py    # –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
‚îÇ       ‚îî‚îÄ‚îÄ test/
‚îÇ
‚îî‚îÄ‚îÄ memory/
    ‚îú‚îÄ‚îÄ memory-bank/               # –ë–∞–Ω–∫ –ø–∞–º—è—Ç–∏ –ø—Ä–æ–µ–∫—Ç–∞
    ‚îÇ   ‚îú‚îÄ‚îÄ projectbrief.md
    ‚îÇ   ‚îú‚îÄ‚îÄ activeContext.md
    ‚îÇ   ‚îú‚îÄ‚îÄ progress.md
    ‚îÇ   ‚îú‚îÄ‚îÄ techContext.md
    ‚îÇ   ‚îî‚îÄ‚îÄ systemPatterns.md
    ‚îî‚îÄ‚îÄ rules/
        ‚îî‚îÄ‚îÄ memory-bank.mdc        # –í—ã—É—á–µ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
```

---

## üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ

### –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ç–µ—Å—Ç–æ–≤

```
–í—Å–µ–≥–æ —Ç–µ—Å—Ç–æ–≤: 70+
–£—Å–ø–µ—à–Ω–æ:      70+ (100%)
–ü–æ–∫—Ä—ã—Ç–∏–µ:     –í—Å–µ –∫–ª—é—á–µ–≤—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã

Breakdown:
‚îú‚îÄ‚îÄ RMSNorm:          ‚úÖ –í—Å–µ —Ç–µ—Å—Ç—ã
‚îú‚îÄ‚îÄ RoPE:             ‚úÖ –í—Å–µ —Ç–µ—Å—Ç—ã
‚îú‚îÄ‚îÄ SwiGLU:           ‚úÖ –í—Å–µ —Ç–µ—Å—Ç—ã
‚îú‚îÄ‚îÄ GQA:              ‚úÖ 12/12
‚îú‚îÄ‚îÄ TransformerBlock: ‚úÖ 14/14
‚îú‚îÄ‚îÄ MoE Router:       ‚úÖ 15/15
‚îî‚îÄ‚îÄ Expert Network:   ‚úÖ 15/15
```

### –¢–∏–ø—ã —Ç–µ—Å—Ç–æ–≤

- ‚úÖ **–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è**: –≤–∞–ª–∏–¥–∞—Ü–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å –∞—Ç—Ä–∏–±—É—Ç–æ–≤
- ‚úÖ **Forward pass**: —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ —Ç–µ–Ω–∑–æ—Ä–æ–≤, –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å –≤—ã—á–∏—Å–ª–µ–Ω–∏–π
- ‚úÖ **Gradient flow**: —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
- ‚úÖ **–ß–∏—Å–ª–µ–Ω–Ω–∞—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å**: –±–æ–ª—å—à–∏–µ/–º–∞–ª–µ–Ω—å–∫–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è
- ‚úÖ **–î–µ—Ç–µ—Ä–º–∏–Ω–∏–∑–º**: –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
- ‚úÖ **–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è**: –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤

---

## üìö –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è

### –ü–æ–¥—Ä–æ–±–Ω—ã–µ –≥–∞–π–¥—ã

- **[SwiGLU.md](experiments/domain/activations/SwiGLU.md)** - –ê–∫—Ç–∏–≤–∞—Ü–∏–æ–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Å –ø—Ä–∏–º–µ—Ä–∞–º–∏
- **[GQA.md](experiments/domain/attention/GQA.md)** - Grouped-Query Attention –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞
- **[GQA_Forward_Explained.md](experiments/domain/attention/GQA_Forward_Explained.md)** - –ü–æ—Å—Ç—Ä–æ—á–Ω–æ–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ forward pass
- **[MoE_Router_Gate_Initialization.md](experiments/domain/moe/MoE_Router_Gate_Initialization.md)** - –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è gate layer (885 —Å—Ç—Ä–æ–∫)
- **[MoE_Router_Load_Balancing_Loss.md](experiments/domain/moe/MoE_Router_Load_Balancing_Loss.md)** - Load balancing loss (1067 —Å—Ç—Ä–æ–∫)

### –ö–ª—é—á–µ–≤—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏

#### Grouped-Query Attention (GQA)
–°–Ω–∏–∂–∞–µ—Ç —Ä–∞–∑–º–µ—Ä KV cache –∑–∞ —Å—á—ë—Ç –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏ –∑–∞–ø—Ä–æ—Å–æ–≤:
```
Query:  8 –≥—Ä—É–ø–ø √ó 64 dim = 512 –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
Key+Value: 16 –≥–æ–ª–æ–≤ √ó 64 dim √ó 2 = 2048 –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
–°–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ KV/Q = 4.0x
```

#### Load Balancing Loss
–ü—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç "–∫–æ–ª–ª–∞–ø—Å —ç–∫—Å–ø–µ—Ä—Ç–æ–≤":
```python
L = Œ± * N * Œ£(f_i * P_i)
–≥–¥–µ:
  f_i = frequency (–∫–∞–∫ —á–∞—Å—Ç–æ —ç–∫—Å–ø–µ—Ä—Ç –≤—ã–±–∏—Ä–∞–µ—Ç—Å—è)
  P_i = mean probability (—Å—Ä–µ–¥–Ω—è—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏)
  Œ± = balance_loss_coef (0.01)
  N = num_experts (8)
```

---

## üéì –û–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω–∞—è —Ü–µ–Ω–Ω–æ—Å—Ç—å

### –î–ª—è –∫–æ–≥–æ —ç—Ç–æ—Ç –ø—Ä–æ–µ–∫—Ç

- üìö **–°—Ç—É–¥–µ–Ω—Ç—ã ML/DL** - –∏–∑—É—á–µ–Ω–∏–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã Transformer –∏ MoE —Å –Ω—É–ª—è
- üî¨ **–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏** - —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å MoE –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞–º–∏
- üë®‚Äçüíª **ML –∏–Ω–∂–µ–Ω–µ—Ä—ã** - –ø–æ–Ω–∏–º–∞–Ω–∏–µ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–≥–æ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞ LLM
- üè´ **–ü—Ä–µ–ø–æ–¥–∞–≤–∞—Ç–µ–ª–∏** - —É—á–µ–±–Ω—ã–π –º–∞—Ç–µ—Ä–∏–∞–ª —Å TODO-—à–∞–±–ª–æ–Ω–∞–º–∏

### –ü–µ–¥–∞–≥–æ–≥–∏—á–µ—Å–∫–∏–π –ø–æ–¥—Ö–æ–¥

- ‚úÖ **TODO-driven development** - –ø–æ—à–∞–≥–æ–≤–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è
- ‚úÖ **–í–æ–ø—Ä–æ—Å—ã –¥–ª—è —Ä–∞–∑–º—ã—à–ª–µ–Ω–∏—è** - —Ä–∞–∑–≤–∏—Ç–∏–µ –ø–æ–Ω–∏–º–∞–Ω–∏—è
- ‚úÖ **–ü–æ–¥—Ä–æ–±–Ω—ã–µ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏** - –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ –∫–∞–∂–¥–æ–π —Å—Ç—Ä–æ–∫–∏
- ‚úÖ **–ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ —Ñ–æ—Ä–º—É–ª—ã** - —Ç–µ–æ—Ä–∏—è + –ø—Ä–∞–∫—Ç–∏–∫–∞
- ‚úÖ **–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏** - ASCII-art –¥–∏–∞–≥—Ä–∞–º–º—ã –ø—Ä–æ—Ü–µ—Å—Å–æ–≤

---

## üîç –ö–ª—é—á–µ–≤—ã–µ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏

### 1. RMSNorm vs LayerNorm
```python
# RMSNorm: x / sqrt(mean(x¬≤) + eps) * weight
# –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:
# - –ù–µ—Ç —Ü–µ–Ω—Ç—Ä–∏—Ä–æ–≤–∞–Ω–∏—è (–Ω–µ –≤—ã—á–∏—Ç–∞–µ—Ç —Å—Ä–µ–¥–Ω–µ–µ)
# - –õ—É—á—à–∞—è —á–∏—Å–ª–µ–Ω–Ω–∞—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å
# - –ú–µ–Ω—å—à–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π
# - –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ LLaMA, Qwen
```

### 2. RoPE Position Encoding
```python
# Rotary Position Embedding
# - –í—Ä–∞—â–µ–Ω–∏–µ –≤ –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–π –ø–ª–æ—Å–∫–æ—Å—Ç–∏
# - –û—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ–µ –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ
# - –•–æ—Ä–æ—à–æ —ç–∫—Å—Ç—Ä–∞–ø–æ–ª–∏—Ä—É–µ—Ç—Å—è –Ω–∞ –¥–ª–∏–Ω–Ω—ã–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
```

### 3. SwiGLU Activation
```python
# SwiGLU(x, W1, W2) = Swish(W1¬∑x) ‚äô (W2¬∑x)
# –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ PaLM, LLaMA, Qwen3
# –õ—É—á—à–µ —á–µ–º ReLU/GELU –≤ –≥–ª—É–±–æ–∫–∏—Ö –º–æ–¥–µ–ª—è—Ö
```

### 4. MoE Router
```python
# Top-K gating —Å load balancing
# 1. Gate projection: logits = Linear(hidden_states)
# 2. Softmax: probabilities = softmax(logits)
# 3. Top-K: –≤—ã–±–æ—Ä K –ª—É—á—à–∏—Ö —ç–∫—Å–ø–µ—Ä—Ç–æ–≤
# 4. Re-normalization: normalize(selected weights)
# 5. Balance loss: –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏–µ –∫–æ–ª–ª–∞–ø—Å–∞ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤
```

---

## ü§ù –í–∫–ª–∞–¥ –≤ –ø—Ä–æ–µ–∫—Ç

–≠—Ç–æ—Ç –ø—Ä–æ–µ–∫—Ç —Å–æ–∑–¥–∞–Ω –≤ –æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö —Ü–µ–ª—è—Ö. –ï—Å–ª–∏ –≤—ã —Ö–æ—Ç–∏—Ç–µ –≤–Ω–µ—Å—Ç–∏ –≤–∫–ª–∞–¥:

1. üêõ **–ù–∞–π–¥–µ–Ω–∞ –æ—à–∏–±–∫–∞?** –û—Ç–∫—Ä–æ–π—Ç–µ issue —Å –ø–æ–¥—Ä–æ–±–Ω—ã–º –æ–ø–∏—Å–∞–Ω–∏–µ–º
2. üí° **–ò–¥–µ—è —É–ª—É—á—à–µ–Ω–∏—è?** –ü—Ä–µ–¥–ª–æ–∂–∏—Ç–µ –≤ discussions
3. üìù **–£–ª—É—á—à–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏?** Pull request –ø—Ä–∏–≤–µ—Ç—Å—Ç–≤—É–µ—Ç—Å—è
4. ‚ú® **–ù–æ–≤—ã–π –∫–æ–º–ø–æ–Ω–µ–Ω—Ç?** –°–ª–µ–¥—É–π—Ç–µ —Å—Ç–∏–ª—é –ø—Ä–æ–µ–∫—Ç–∞

---

## üìñ –û–±—É—á–∞—é—â–∏–µ –º–∞—Ç–µ—Ä–∏–∞–ª—ã

### –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–π –ø–æ—Ä—è–¥–æ–∫ –∏–∑—É—á–µ–Ω–∏—è

1. **–ë–∞–∑–æ–≤—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã**:
   - RMSNorm ‚Üí RoPE ‚Üí SwiGLU
   - Grouped-Query Attention
   - TransformerBlock

2. **MoE –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã**:
   - MoE Router (gating mechanism)
   - Expert Network (feed-forward)
   - MoE Layer (–∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è)

3. **–ü–æ–ª–Ω–∞—è –º–æ–¥–µ–ª—å**:
   - –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
   - Embedding + N√óBlocks + LM Head
   - –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞

### –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ

```bash
# –ó–∞–ø—É—Å—Ç–∏—Ç–µ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
python3 experiments/domain/moe/test_integration.py

# –í—ã —É–≤–∏–¥–∏—Ç–µ:
# - –ö–∞–∫ Router –≤—ã–±–∏—Ä–∞–µ—Ç —ç–∫—Å–ø–µ—Ä—Ç–æ–≤
# - –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–æ–≤ –ø–æ —ç–∫—Å–ø–µ—Ä—Ç–∞–º
# - –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—é load balancing
# - –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è –¥–ª—è MoE Layer
```

---

## üìà –ú–µ—Ç—Ä–∏–∫–∏ –∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞

### –ü—Ä–æ–≥—Ä–µ—Å—Å –ø—Ä–æ–µ–∫—Ç–∞
```
–û–±—â–∏–π –ø—Ä–æ–≥—Ä–µ—Å—Å:    60%
–°—Ç—Ä–æ–∫ –∫–æ–¥–∞:        ~3500+ (—Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è)
–°—Ç—Ä–æ–∫ —Ç–µ—Å—Ç–æ–≤:      ~2500+
–î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è:      ~6000+ —Å—Ç—Ä–æ–∫ –≤ .md —Ñ–∞–π–ª–∞—Ö
–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏:       –ü–æ–¥—Ä–æ–±–Ω—ã–µ –≤ –∫–∞–∂–¥–æ–º —Ñ–∞–π–ª–µ
```

### –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å
```
–†–∞–∑–º–µ—Ä –º–æ–¥–µ–ª–∏:     0.6B –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
–ê–∫—Ç–∏–≤–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: ~0.15-0.2B (25-33%)
–ü–∞–º—è—Ç—å:            –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–æ —á–µ—Ä–µ–∑ GQA
–¢–µ—Å—Ç—ã:             –í—Å–µ –ø—Ä–æ—Ö–æ–¥—è—Ç –∑–∞ ~5 —Å–µ–∫—É–Ω–¥
```

---

## üîó –°—Å—ã–ª–∫–∏ –∏ —Ä–µ—Å—É—Ä—Å—ã

### –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞
- [Qwen3 Technical Report](https://arxiv.org/abs/2409.12186)
- [Mixture-of-Experts with Expert Choice Routing](https://arxiv.org/abs/2202.09368)
- [RoFormer: Enhanced Transformer with Rotary Position Embedding](https://arxiv.org/abs/2104.09864)
- [GQA: Training Generalized Multi-Query Transformer Models](https://arxiv.org/abs/2305.13245)

### –†–µ–∞–ª–∏–∑–∞—Ü–∏–∏
- [Hugging Face Transformers](https://github.com/huggingface/transformers)
- [PyTorch](https://pytorch.org/)

---

## üìù –õ–∏—Ü–µ–Ω–∑–∏—è

–≠—Ç–æ—Ç –ø—Ä–æ–µ–∫—Ç —Å–æ–∑–¥–∞–Ω –≤ –æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö —Ü–µ–ª—è—Ö. –ö–æ–¥ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç—Å—è "–∫–∞–∫ –µ—Å—Ç—å" –¥–ª—è –∏–∑—É—á–µ–Ω–∏—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã MoE Transformer.

---

## üôè –ë–ª–∞–≥–æ–¥–∞—Ä–Ω–æ—Å—Ç–∏

–û—Å–æ–±–∞—è –±–ª–∞–≥–æ–¥–∞—Ä–Ω–æ—Å—Ç—å:
- –ö–æ–º–∞–Ω–¥–µ Qwen3 –∑–∞ –æ—Ç–∫—Ä—ã—Ç—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É
- –°–æ–æ–±—â–µ—Å—Ç–≤—É PyTorch –∑–∞ –æ—Ç–ª–∏—á–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫
- –í—Å–µ–º –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—è–º –≤ –æ–±–ª–∞—Å—Ç–∏ MoE –∏ Transformers

---

<div align="center">

**Made with ‚ù§Ô∏è for ML Education**

‚≠ê Star this repo if you found it helpful!

</div>

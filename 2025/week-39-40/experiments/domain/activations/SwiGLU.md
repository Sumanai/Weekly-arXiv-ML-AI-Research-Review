# SwiGLU — Swish-Gated Linear Unit

## 1. Что такое активационные функции

Активационные функции в нейронных сетях вводят нелинейность, позволяя моделям изучать сложные паттерны. Без них многослойная нейронная сеть эквивалентна одному линейному преобразованию.

Основные свойства хороших функций активации:
- **Нелинейность**: позволяет моделировать сложные зависимости;
- **Дифференцируемость**: необходима для обратного распространения ошибки;
- **Монотонность**: упрощает оптимизацию;
- **Диапазон выходных значений**: влияет на стабильность обучения.

---

## 2. GELU (Gaussian Error Linear Unit)

## Теоретические основы

### Формула GELU

GELU определяется как:

$$\text{GELU}(x) = x \cdot \Phi(x)$$

где $\Phi(x)$ — кумулятивная функция распределения стандартного нормального закона.

### Что такое $\Phi(x)$?

$\Phi(x)$ — это **кумулятивная функция распределения (CDF)** стандартного нормального распределения $N(0,1)$. Она показывает вероятность того, что случайная величина $z \sim N(0,1)$ примет значение меньше или равно $x$:

$$\Phi(x) = P(z \leq x), \quad z \sim N(0,1)$$

Геометрически это «площадь под колоколообразной кривой Гаусса» от $-\infty$ до $x$.

## Концепция пробабилистического гейта

### Что означает "вход x"?

Под $x$ подразумевается **входной тензор слоя нейросети**. Если у нас трансформер, то это, как правило, **матрица эмбеддингов токенов** размером `(batch_size, seq_len, hidden_dim)`.

GELU применяется **покомпонентно** к каждому элементу тензора:

$$Y_{i,j,k} = \text{GELU}(X_{i,j,k})$$

### Механизм пробабилистического гейта

GELU можно понимать как **мягкий вероятностный фильтр**:

- Обычная линейная функция: $f(x) = x$
- В GELU она модифицируется: $f(x) = x \cdot \Phi(x)$

**Смысл**: мы «масштабируем» значение $x$ вероятностью того, что случайная величина из нормального распределения окажется **меньше $x$**.

**Поведение функции:**
- Если $x \gg 0$, то $\Phi(x) \approx 1$, значит $\text{GELU}(x) \approx x$
- Если $x \ll 0$, то $\Phi(x) \approx 0$, значит $\text{GELU}(x) \approx 0$
- Если $x$ близко к 0, то вероятность $\Phi(x) \approx 0.5$, и значение частично приглушается

**Гейт (gate)** в нейросетях — это механизм, который решает, пропустить ли информацию дальше или подавить её. В GELU гейт не бинарный, а «вероятностный»: каждое значение $x$ «взвешивается» вероятностью $\Phi(x)$.

## Практический пример

Рассмотрим применение GELU к матрице эмбеддингов размера $4 \times 4$ (4 токена × 4 признака):

### Исходные эмбеддинги

```
X:
"Я":         [0.2, 0.5, 0.1, 0.7]
"люблю":     [0.3, 0.6, 0.0, 0.8]
"машинное":  [0.9, 0.1, 0.4, 0.3]
"обучение":  [0.5, 0.2, 0.9, 0.6]
```

### Шаг 1: Вычисление значений $\Phi(x)$

Для всех встречающихся значений:

| x    | 0.0   | 0.1   | 0.2   | 0.3   | 0.4   | 0.5   | 0.6   | 0.7   | 0.8   | 0.9   |
|------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|
| Φ(x) | 0.5000| 0.5398| 0.5793| 0.6179| 0.6554| 0.6915| 0.7257| 0.7580| 0.7881| 0.8159|

### Шаг 2: Применение GELU покомпонентно

Для каждого токена умножаем $x$ на соответствующее $\Phi(x)$:

#### "Я" — [0.2, 0.5, 0.1, 0.7]
- $\Phi$ → [0.5793, 0.6915, 0.5398, 0.7580]
- **Результат** → [0.116, 0.346, 0.054, 0.531]

#### "люблю" — [0.3, 0.6, 0.0, 0.8]
- $\Phi$ → [0.6179, 0.7257, 0.5000, 0.7881]
- **Результат** → [0.185, 0.435, 0.000, 0.631]

#### "машинное" — [0.9, 0.1, 0.4, 0.3]
- $\Phi$ → [0.8159, 0.5398, 0.6554, 0.6179]
- **Результат** → [0.734, 0.054, 0.262, 0.185]

#### "обучение" — [0.5, 0.2, 0.9, 0.6]
- $\Phi$ → [0.6915, 0.5793, 0.8159, 0.7257]
- **Результат** → [0.346, 0.116, 0.734, 0.435]

### Итоговый результат

```
Y = GELU(X):
"Я":         [0.116, 0.346, 0.054, 0.531]
"люблю":     [0.185, 0.435, 0.000, 0.631]
"машинное":  [0.734, 0.054, 0.262, 0.185]
"обучение":  [0.346, 0.116, 0.734, 0.435]
```

## Интерпретация результатов

### Механизм "мягкого взвешивания"

- **Коэффициент $\Phi(x)$** — это доля, на которую мы пропускаем компоненту
- Для $x=0.1$: гейт открыт на ~54% → $0.1$ превращается в $0.054$
- Для $x=0.9$: гейт открыт на ~82% → $0.9$ становится $0.734$

### Ключевые свойства

1. **Каждая компонента «пропускается» пропорционально своей величине**
2. **Положительные значения уменьшаются**, но **монотонность сохраняется**
3. **Нули остаются нулями**: $0 \cdot \Phi(0) = 0$
4. **Малые положительные значения приглушаются сильнее** (в окрестности нуля $\Phi(x) \approx 0.5$)
5. **Крупные положительные почти проходят** ($\Phi(x)$ растёт к 1, но остаётся < 1)

### Аналогия с физическим гейтом

Можно представить как:
- Входное число $x$ = «сила сигнала»
- $\Phi(x)$ = «насколько открыт гейт» (от 0 до 1)
- GELU = сигнал × степень открытия

## Преимущества GELU

1. **Мягкость**: В отличие от ReLU, GELU не обрубает сигналы резко, а мягко подавляет их
2. **Вероятностная интерпретация**: Каждое значение проходит через «заслонку», открытую пропорционально вероятности
3. **Плавные градиенты**: Обеспечивает более устойчивое обучение
4. **Динамическое сжатие**: Уменьшает шумовые слабые активации, оставляя сильные более заметными

## Заключение

GELU представляет собой элегантное сочетание линейной функции и вероятностного гейта. Это делает её мощным инструментом для современных архитектур нейронных сетей, особенно трансформеров, где требуется мягкое, но эффективное управление потоком информации через слои сети.

В терминах «пробабилистического гейта»: каждое $x$ проходит через «заслонку», открытую на $\Phi(x)$. Чем больше $x$, тем шире открыта заслонка и тем сильнее сигнал проходит дальше.

---

## 3. Swish

## Теоретические основы

### Формула Swish

Swish определяется как:

$$\text{Swish}(x) = x \cdot \sigma(\beta x)$$

где:
- $\sigma(z) = \frac{1}{1 + e^{-z}}$ — сигмоидная функция
- $\beta$ — параметр масштабирования (обычно $\beta = 1$)

При $\beta = 1$ формула упрощается до:

$$\text{Swish}(x) = x \cdot \sigma(x) = \frac{x}{1 + e^{-x}}$$

### Что такое сигмоидная функция?

Сигмоидная функция $\sigma(x) = \frac{1}{1 + e^{-x}}$ — это S-образная кривая, которая:
- Принимает значения в диапазоне $(0, 1)$
- При $x \to +\infty$: $\sigma(x) \to 1$
- При $x \to -\infty$: $\sigma(x) \to 0$
- При $x = 0$: $\sigma(0) = 0.5$

## Концепция самогейтинга (Self-Gating)

### Механизм работы

В отличие от GELU, где «гейт» задается внешней функцией $\Phi(x)$, в Swish **сам вход $x$ определяет степень своего прохождения**:

- **Линейная часть**: $x$ (исходный сигнал)
- **Гейт**: $\sigma(x)$ (зависит от того же самого $x$)
- **Результат**: $x \cdot \sigma(x)$ (самомодуляция)

### Поведение функции

**Для больших положительных значений** ($x \gg 0$):
- $\sigma(x) \approx 1$
- $\text{Swish}(x) \approx x \cdot 1 = x$ (ведёт себя как линейная функция)

**Для больших отрицательных значений** ($x \ll 0$):
- $\sigma(x) \approx 0$
- $\text{Swish}(x) \approx x \cdot 0 = 0$ (подавляется, но не резко)

**В окрестности нуля**:
- $\sigma(0) = 0.5$
- $\text{Swish}(0) = 0 \cdot 0.5 = 0$
- Функция плавно переходит через начало координат

## Детальный анализ свойств

### 1. Гладкость и дифференцируемость

**Производная Swish**:
$$\frac{d}{dx}\text{Swish}(x) = \sigma(x) + x \cdot \sigma(x)(1 - \sigma(x))$$

Это можно переписать как:
$$\text{Swish}'(x) = \sigma(x)(1 + x(1 - \sigma(x)))$$

**Ключевые особенности**:
- Функция **бесконечно дифференцируема** (гладкая)
- Производная никогда не равна нулю для положительных $x$
- Нет проблемы «умирающих нейронов»

### 2. Ограниченность

**Снизу**: Функция имеет **глобальный минимум** около $x \approx -1.278$ со значением $\approx -0.278$

**Сверху**: Функция **не ограничена сверху** — при $x \to +\infty$ имеем $\text{Swish}(x) \to +\infty$

## Практический пример

Рассмотрим применение Swish к той же матрице эмбеддингов $4 \times 4$:

### Исходные эмбеддинги

```
X:
"Я":         [0.2, 0.5, 0.1, 0.7]
"люблю":     [0.3, 0.6, 0.0, 0.8]
"машинное":  [0.9, 0.1, 0.4, 0.3]
"обучение":  [0.5, 0.2, 0.9, 0.6]
```

### Шаг 1: Вычисление сигмоидных значений σ(x)

Для всех встречающихся значений:

| x    | 0.0   | 0.1   | 0.2   | 0.3   | 0.4   | 0.5   | 0.6   | 0.7   | 0.8   | 0.9   |
|------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|
| σ(x) | 0.5000| 0.5250| 0.5498| 0.5744| 0.5987| 0.6225| 0.6457| 0.6682| 0.6900| 0.7109|

**Формула для вычисления**: $\sigma(x) = \frac{1}{1 + e^{-x}}$

### Шаг 2: Применение Swish покомпонентно

Для каждого токена умножаем $x$ на соответствующее $\sigma(x)$:

#### "Я" — [0.2, 0.5, 0.1, 0.7]
- $\sigma$ → [0.5498, 0.6225, 0.5250, 0.6682]
- **Результат** → [0.110, 0.311, 0.053, 0.468]

#### "люблю" — [0.3, 0.6, 0.0, 0.8]
- $\sigma$ → [0.5744, 0.6457, 0.5000, 0.6900]
- **Результат** → [0.172, 0.387, 0.000, 0.552]

#### "машинное" — [0.9, 0.1, 0.4, 0.3]
- $\sigma$ → [0.7109, 0.5250, 0.5987, 0.5744]
- **Результат** → [0.640, 0.053, 0.239, 0.172]

#### "обучение" — [0.5, 0.2, 0.9, 0.6]
- $\sigma$ → [0.6225, 0.5498, 0.7109, 0.6457]
- **Результат** → [0.311, 0.110, 0.640, 0.387]

### Итоговый результат

```
Y = Swish(X):
"Я":         [0.110, 0.311, 0.053, 0.468]
"люблю":     [0.172, 0.387, 0.000, 0.552]
"машинное":  [0.640, 0.053, 0.239, 0.172]
"обучение":  [0.311, 0.110, 0.640, 0.387]
```

## Интерпретация результатов

### Механизм самомодуляции

- **Каждое значение $x$ само определяет степень своего прохождения** через $\sigma(x)$
- Для $x=0.1$: сигмоид = 52.5% → результат $0.053$ (сильное приглушение)
- Для $x=0.9$: сигмоид = 71.1% → результат $0.640$ (умеренное ослабление)

### Ключевые наблюдения

1. **Все положительные значения уменьшаются**, но пропорционально
2. **Монотонность сохраняется**: больший $x$ даёт больший результат
3. **Нули остаются нулями**: $0 \cdot \sigma(0) = 0$
4. **Нет резких переходов**: функция везде гладкая

## Поведение в различных диапазонах

### Для отрицательных значений

Рассмотрим пример с отрицательными компонентами:

| x     | -2.0  | -1.0  | -0.5  | -0.1  |
|-------|-------|-------|-------|-------|
| σ(x)  | 0.119 | 0.269 | 0.378 | 0.475 |
| Swish | -0.238| -0.269| -0.189| -0.048|

**Важно**: Отрицательные значения **не обнуляются полностью**, а мягко подавляются. Minimum функции достигается при $x \approx -1.278$ со значением $\approx -0.278$.

### Сравнение с ReLU и GELU

Для $x = 0.5$:
- **ReLU**: $\max(0, 0.5) = 0.5$ (без изменений)
- **GELU**: $0.5 \cdot \Phi(0.5) = 0.5 \cdot 0.6915 = 0.346$
- **Swish**: $0.5 \cdot \sigma(0.5) = 0.5 \cdot 0.6225 = 0.311$

## Влияние параметра β

### Роль коэффициента β

При использовании $\text{Swish}(x) = x \cdot \sigma(\beta x)$:

- **β = 0**: Функция становится $\frac{x}{2}$ (линейная с коэффициентом 0.5)
- **β → ∞**: Функция приближается к ReLU
- **β = 1**: Стандартный случай (самый популярный)

### Настройка чувствительности

Больший $\beta$ делает переход от подавления к активации **более резким**:
- При больших $\beta$ поведение ближе к ReLU
- При малых $\beta$ функция более «мягкая»

## Преимущества Swish

### 1. Отсутствие проблемы умирающих нейронов
- Производная никогда не равна нулю для $x > 0$
- Градиенты всегда присутствуют для обратного распространения

### 2. Гладкость и стабильность
- Бесконечная дифференцируемость обеспечивает стабильную оптимизацию
- Нет разрывов и изломов как в ReLU

### 3. Самоадаптивность
- Функция **сама регулирует** степень активации на основе входного сигнала
- Не требует дополнительных параметров (при β = 1)

### 4. Эмпирически лучшие результаты
- Во многих задачах показывает лучшую производительность по сравнению с ReLU
- Особенно эффективна в глубоких сетях

## Недостатки и ограничения

### 1. Вычислительная сложность
- Требует вычисления экспоненты $e^{-x}$
- Медленнее ReLU, но быстрее GELU

### 2. Ненулевой минимум
- Функция может принимать отрицательные значения (до -0.278)
- Может влиять на стабильность в некоторых архитектурах

## Заключение

Swish представляет собой элегантную функцию активации, которая сочетает простоту формулировки с мощными свойствами. Ключевая идея **самогейтинга** — когда вход сам определяет степень своего прохождения — делает функцию адаптивной и эффективной.

**Основные характеристики**:
- **Гладкость**: обеспечивает стабильные градиенты
- **Самомодуляция**: вход регулирует собственную активацию
- **Монотонность**: сохраняет упорядоченность значений
- **Мягкость**: нет резких обрывов как в ReLU

Swish особенно эффективна в современных глубоких архитектурах, где требуется баланс между вычислительной эффективностью и качеством градиентов.

### Реализация в PyTorch

```python
import torch.nn as nn
import torch.nn.functional as F

x = ...
y = F.silu(x)  # Swish = SiLU
```

---

## 4. SwiGLU

### Формула

$$
SwiGLU(x, W_1, W_2, b_1, b_2) = Swish(W_1x + b_1) \odot (W_2x + b_2)
$$

где:

* $W_1, W_2$ — весовые матрицы,
* $b_1, b_2$ — смещения,
* $\odot$ — поэлементное умножение.

### Особенности

* Обычно используется с промежуточной размерностью:
  $\text{intermediate\_dim} = 4 * \text{hidden\_size}$.
* Совмещает **гладкость Swish** и **гейтинг GLU**.

### Преимущества

1. Улучшает производительность трансформеров.
2. Контролирует поток информации (через гейтинг).
3. Обеспечивает стабильное обучение.
4. Повышает выразительность модели.

### Применение

Используется в **PaLM**, **LLaMA**, **Qwen3** и других современных LLM.

### Реализация в PyTorch

```python
class SwiGLU(nn.Module):
    def __init__(self, input_dim, output_dim, intermediate_dim=None):
        super().__init__()
        if intermediate_dim is None:
            intermediate_dim = 4 * output_dim
        self.gate_proj = nn.Linear(input_dim, intermediate_dim)
        self.value_proj = nn.Linear(input_dim, intermediate_dim)
        self.output_proj = nn.Linear(intermediate_dim, output_dim)

    def forward(self, x):
        gate = F.silu(self.gate_proj(x))  # Swish/SiLU
        value = self.value_proj(x)
        return self.output_proj(gate * value)
```

---

## 5. Итог

* **GELU**: плавная альтернатива ReLU, работает лучше в трансформерах.
* **Swish**: сглаженная версия ReLU с мягким подавлением отрицательных значений.
* **SwiGLU**: современное сочетание Swish и гейтинга, стандарт в LLM.
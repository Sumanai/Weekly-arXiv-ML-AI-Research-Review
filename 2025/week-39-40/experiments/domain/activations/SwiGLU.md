# SwiGLU — Swish-Gated Linear Unit

## 1. Что такое активационные функции

Активационные функции в нейронных сетях вводят нелинейность, позволяя моделям изучать сложные паттерны. Без них многослойная нейронная сеть эквивалентна одному линейному преобразованию.

Основные свойства хороших функций активации:
- **Нелинейность**: позволяет моделировать сложные зависимости;
- **Дифференцируемость**: необходима для обратного распространения ошибки;
- **Монотонность**: упрощает оптимизацию;
- **Диапазон выходных значений**: влияет на стабильность обучения.

---

## 2. GELU (Gaussian Error Linear Unit)

## Теоретические основы

### Формула GELU

GELU определяется как:

$$\text{GELU}(x) = x \cdot \Phi(x)$$

где $\Phi(x)$ — кумулятивная функция распределения стандартного нормального закона.

### Что такое $\Phi(x)$?

$\Phi(x)$ — это **кумулятивная функция распределения (CDF)** стандартного нормального распределения $N(0,1)$. Она показывает вероятность того, что случайная величина $z \sim N(0,1)$ примет значение меньше или равно $x$:

$$\Phi(x) = P(z \leq x), \quad z \sim N(0,1)$$

Геометрически это «площадь под колоколообразной кривой Гаусса» от $-\infty$ до $x$.

## Концепция пробабилистического гейта

### Что означает "вход x"?

Под $x$ подразумевается **входной тензор слоя нейросети**. Если у нас трансформер, то это, как правило, **матрица эмбеддингов токенов** размером `(batch_size, seq_len, hidden_dim)`.

GELU применяется **покомпонентно** к каждому элементу тензора:

$$Y_{i,j,k} = \text{GELU}(X_{i,j,k})$$

### Механизм пробабилистического гейта

GELU можно понимать как **мягкий вероятностный фильтр**:

- Обычная линейная функция: $f(x) = x$
- В GELU она модифицируется: $f(x) = x \cdot \Phi(x)$

**Смысл**: мы «масштабируем» значение $x$ вероятностью того, что случайная величина из нормального распределения окажется **меньше $x$**.

**Поведение функции:**
- Если $x \gg 0$, то $\Phi(x) \approx 1$, значит $\text{GELU}(x) \approx x$
- Если $x \ll 0$, то $\Phi(x) \approx 0$, значит $\text{GELU}(x) \approx 0$
- Если $x$ близко к 0, то вероятность $\Phi(x) \approx 0.5$, и значение частично приглушается

**Гейт (gate)** в нейросетях — это механизм, который решает, пропустить ли информацию дальше или подавить её. В GELU гейт не бинарный, а «вероятностный»: каждое значение $x$ «взвешивается» вероятностью $\Phi(x)$.

## Практический пример

Рассмотрим применение GELU к матрице эмбеддингов размера $4 \times 4$ (4 токена × 4 признака):

### Исходные эмбеддинги

```
X:
"Я":         [0.2, 0.5, 0.1, 0.7]
"люблю":     [0.3, 0.6, 0.0, 0.8]
"машинное":  [0.9, 0.1, 0.4, 0.3]
"обучение":  [0.5, 0.2, 0.9, 0.6]
```

### Шаг 1: Вычисление значений $\Phi(x)$

Для всех встречающихся значений:

| x    | 0.0   | 0.1   | 0.2   | 0.3   | 0.4   | 0.5   | 0.6   | 0.7   | 0.8   | 0.9   |
|------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|
| Φ(x) | 0.5000| 0.5398| 0.5793| 0.6179| 0.6554| 0.6915| 0.7257| 0.7580| 0.7881| 0.8159|

### Шаг 2: Применение GELU покомпонентно

Для каждого токена умножаем $x$ на соответствующее $\Phi(x)$:

#### "Я" — [0.2, 0.5, 0.1, 0.7]
- $\Phi$ → [0.5793, 0.6915, 0.5398, 0.7580]
- **Результат** → [0.116, 0.346, 0.054, 0.531]

#### "люблю" — [0.3, 0.6, 0.0, 0.8]
- $\Phi$ → [0.6179, 0.7257, 0.5000, 0.7881]
- **Результат** → [0.185, 0.435, 0.000, 0.631]

#### "машинное" — [0.9, 0.1, 0.4, 0.3]
- $\Phi$ → [0.8159, 0.5398, 0.6554, 0.6179]
- **Результат** → [0.734, 0.054, 0.262, 0.185]

#### "обучение" — [0.5, 0.2, 0.9, 0.6]
- $\Phi$ → [0.6915, 0.5793, 0.8159, 0.7257]
- **Результат** → [0.346, 0.116, 0.734, 0.435]

### Итоговый результат

```
Y = GELU(X):
"Я":         [0.116, 0.346, 0.054, 0.531]
"люблю":     [0.185, 0.435, 0.000, 0.631]
"машинное":  [0.734, 0.054, 0.262, 0.185]
"обучение":  [0.346, 0.116, 0.734, 0.435]
```

## Интерпретация результатов

### Механизм "мягкого взвешивания"

- **Коэффициент $\Phi(x)$** — это доля, на которую мы пропускаем компоненту
- Для $x=0.1$: гейт открыт на ~54% → $0.1$ превращается в $0.054$
- Для $x=0.9$: гейт открыт на ~82% → $0.9$ становится $0.734$

### Ключевые свойства

1. **Каждая компонента «пропускается» пропорционально своей величине**
2. **Положительные значения уменьшаются**, но **монотонность сохраняется**
3. **Нули остаются нулями**: $0 \cdot \Phi(0) = 0$
4. **Малые положительные значения приглушаются сильнее** (в окрестности нуля $\Phi(x) \approx 0.5$)
5. **Крупные положительные почти проходят** ($\Phi(x)$ растёт к 1, но остаётся < 1)

### Аналогия с физическим гейтом

Можно представить как:
- Входное число $x$ = «сила сигнала»
- $\Phi(x)$ = «насколько открыт гейт» (от 0 до 1)
- GELU = сигнал × степень открытия

## Преимущества GELU

1. **Мягкость**: В отличие от ReLU, GELU не обрубает сигналы резко, а мягко подавляет их
2. **Вероятностная интерпретация**: Каждое значение проходит через «заслонку», открытую пропорционально вероятности
3. **Плавные градиенты**: Обеспечивает более устойчивое обучение
4. **Динамическое сжатие**: Уменьшает шумовые слабые активации, оставляя сильные более заметными

## Заключение

GELU представляет собой элегантное сочетание линейной функции и вероятностного гейта. Это делает её мощным инструментом для современных архитектур нейронных сетей, особенно трансформеров, где требуется мягкое, но эффективное управление потоком информации через слои сети.

В терминах «пробабилистического гейта»: каждое $x$ проходит через «заслонку», открытую на $\Phi(x)$. Чем больше $x$, тем шире открыта заслонка и тем сильнее сигнал проходит дальше.

---

## 3. Swish

### Формула

$$
Swish(x) = x * sigmoid(βx), \quad β = 1
$$

### Свойства

* Гладкая, все производные существуют.
* Ограничена снизу (≈ -0.278 при β=1).
* Неограничена сверху.
* Ведёт себя похоже на ReLU при больших $x > 0$.
* Подавляет отрицательные значения мягко, а не полностью.

### Преимущества

* Нет проблемы «умирающих нейронов».
* Сохраняет плавные градиенты.
* Обычно работает лучше ReLU в глубоких сетях.

### Реализация в PyTorch

```python
import torch.nn as nn
import torch.nn.functional as F

x = ...
y = F.silu(x)  # Swish = SiLU
```

---

## 4. SwiGLU

### Формула

$$
SwiGLU(x, W_1, W_2, b_1, b_2) = Swish(W_1x + b_1) \odot (W_2x + b_2)
$$

где:

* $W_1, W_2$ — весовые матрицы,
* $b_1, b_2$ — смещения,
* $\odot$ — поэлементное умножение.

### Особенности

* Обычно используется с промежуточной размерностью:
  $\text{intermediate\_dim} = 4 * \text{hidden\_size}$.
* Совмещает **гладкость Swish** и **гейтинг GLU**.

### Преимущества

1. Улучшает производительность трансформеров.
2. Контролирует поток информации (через гейтинг).
3. Обеспечивает стабильное обучение.
4. Повышает выразительность модели.

### Применение

Используется в **PaLM**, **LLaMA**, **Qwen3** и других современных LLM.

### Реализация в PyTorch

```python
class SwiGLU(nn.Module):
    def __init__(self, input_dim, output_dim, intermediate_dim=None):
        super().__init__()
        if intermediate_dim is None:
            intermediate_dim = 4 * output_dim
        self.gate_proj = nn.Linear(input_dim, intermediate_dim)
        self.value_proj = nn.Linear(input_dim, intermediate_dim)
        self.output_proj = nn.Linear(intermediate_dim, output_dim)

    def forward(self, x):
        gate = F.silu(self.gate_proj(x))  # Swish/SiLU
        value = self.value_proj(x)
        return self.output_proj(gate * value)
```

---

## 5. Итог

* **GELU**: плавная альтернатива ReLU, работает лучше в трансформерах.
* **Swish**: сглаженная версия ReLU с мягким подавлением отрицательных значений.
* **SwiGLU**: современное сочетание Swish и гейтинга, стандарт в LLM.
# SwiGLU — Swish-Gated Linear Unit

## 1. Что такое активационные функции

Активационные функции в нейронных сетях вводят нелинейность, позволяя моделям изучать сложные паттерны. Без них многослойная нейронная сеть эквивалентна одному линейному преобразованию.

Основные свойства хороших функций активации:
- **Нелинейность**: позволяет моделировать сложные зависимости;
- **Дифференцируемость**: необходима для обратного распространения ошибки;
- **Монотонность**: упрощает оптимизацию;
- **Диапазон выходных значений**: влияет на стабильность обучения.

---

## 2. GELU (Gaussian Error Linear Unit)

### Формула

$$
GELU(x) = x * Φ(x)
$$

где $Φ(x)$ — кумулятивная функция стандартного нормального распределения.

### Свойства

* Плавные градиенты, в отличие от ReLU.
* Для больших положительных значений ≈ линейная функция.
* Для отрицательных значений даёт малые, но ненулевые значения.

### Применение

Использовалась в **BERT** и ранних версиях **GPT**.

---

## 3. Swish

### Формула

$$
Swish(x) = x * sigmoid(βx), \quad β = 1
$$

### Свойства

* Гладкая, все производные существуют.
* Ограничена снизу (≈ -0.278 при β=1).
* Неограничена сверху.
* Ведёт себя похоже на ReLU при больших $x > 0$.
* Подавляет отрицательные значения мягко, а не полностью.

### Преимущества

* Нет проблемы «умирающих нейронов».
* Сохраняет плавные градиенты.
* Обычно работает лучше ReLU в глубоких сетях.

### Реализация в PyTorch

```python
import torch.nn as nn
import torch.nn.functional as F

x = ...
y = F.silu(x)  # Swish = SiLU
```

---

## 4. SwiGLU

### Формула

$$
SwiGLU(x, W_1, W_2, b_1, b_2) = Swish(W_1x + b_1) \odot (W_2x + b_2)
$$

где:

* $W_1, W_2$ — весовые матрицы,
* $b_1, b_2$ — смещения,
* $\odot$ — поэлементное умножение.

### Особенности

* Обычно используется с промежуточной размерностью:
  $\text{intermediate\_dim} = 4 * \text{hidden\_size}$.
* Совмещает **гладкость Swish** и **гейтинг GLU**.

### Преимущества

1. Улучшает производительность трансформеров.
2. Контролирует поток информации (через гейтинг).
3. Обеспечивает стабильное обучение.
4. Повышает выразительность модели.

### Применение

Используется в **PaLM**, **LLaMA**, **Qwen3** и других современных LLM.

### Реализация в PyTorch

```python
class SwiGLU(nn.Module):
    def __init__(self, input_dim, output_dim, intermediate_dim=None):
        super().__init__()
        if intermediate_dim is None:
            intermediate_dim = 4 * output_dim
        self.gate_proj = nn.Linear(input_dim, intermediate_dim)
        self.value_proj = nn.Linear(input_dim, intermediate_dim)
        self.output_proj = nn.Linear(intermediate_dim, output_dim)

    def forward(self, x):
        gate = F.silu(self.gate_proj(x))  # Swish/SiLU
        value = self.value_proj(x)
        return self.output_proj(gate * value)
```

---

## 5. Итог

* **GELU**: плавная альтернатива ReLU, работает лучше в трансформерах.
* **Swish**: сглаженная версия ReLU с мягким подавлением отрицательных значений.
* **SwiGLU**: современное сочетание Swish и гейтинга, стандарт в LLM.
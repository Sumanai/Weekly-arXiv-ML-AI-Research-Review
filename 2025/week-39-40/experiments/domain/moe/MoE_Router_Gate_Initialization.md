# MoE Router: Инициализация Gate Layer

## Оглавление
- [Обзор](#обзор)
- [Строка 1: Создание Linear Layer](#строка-1-создание-linear-layer)
- [Строка 2: Инициализация весов](#строка-2-инициализация-весов)
- [Строка 3: Инициализация bias](#строка-3-инициализация-bias)
- [Полная картина](#полная-картина)
- [Практический пример](#практический-пример)
- [Полный процесс маршрутизации: От логитов к экспертам](#полный-процесс-маршрутизации-от-логитов-к-экспертам)

---

## Обзор

В методе `__init__` класса `MoERouter` три ключевые строки отвечают за создание и инициализацию gate layer:

```python
self.gate = nn.Linear(hidden_size, num_experts)      # 1. Создание
self.gate.weight.data.normal_(0, 0.01)               # 2. Инициализация весов
if self.gate.bias is not None:                       # 3. Инициализация bias
    self.gate.bias.data.zero_()
```

**Цель gate layer**: Преобразовать входной токен в **логиты (scores)** для каждого эксперта, чтобы затем выбрать Top-K лучших.

### ⚠️ Важное различие: Логиты vs Вероятности

**Логиты (Logits)** — это **сырые выходные значения** gate layer:
- Могут быть любыми числами: положительными, отрицательными, > 1
- НЕ являются вероятностями
- Это просто скоры (scores), показывающие "насколько подходит каждый эксперт"

**Вероятности (Probabilities)** — получаются **после применения Softmax**:
- Все значения в диапазоне [0, 1]
- Сумма всех вероятностей = 1.0
- Корректное распределение вероятностей

```python
# Логиты (до Softmax)
logits = [2.3, -1.5, 0.8, -0.3]  # Любые значения

# Вероятности (после Softmax)
probs = [0.72, 0.02, 0.16, 0.10]  # Сумма = 1.0, все в [0,1]
```

---

## Строка 1: Создание Linear Layer

### Код
```python
self.gate = nn.Linear(hidden_size, num_experts)
```

### Что это делает?

Создает **полносвязный (fully connected) слой**, который выполняет линейное преобразование:

```
gate(x) = W * x + b
```

### Параметры

| Параметр | Значение | Размерность |
|----------|----------|-------------|
| `hidden_size` | 2048 (для Qwen3) | Входная размерность |
| `num_experts` | 128 (для Qwen3) | Выходная размерность |
| `W` (веса) | Обучаемая матрица | `(128, 2048)` |
| `b` (bias) | Обучаемый вектор | `(128,)` |

### Математика

```
Для одного токена x ∈ ℝ^2048:

logits = W · x + b

где:
  W ∈ ℝ^(128×2048)  - матрица весов
  b ∈ ℝ^128         - вектор смещений
  logits ∈ ℝ^128    - score для каждого эксперта
```

### Визуализация

```
┌──────────────┐
│ Input Token  │
│   (2048)     │
└──────┬───────┘
       │
       ▼
┌──────────────────────────┐
│   Gate Layer             │
│                          │
│  W(128×2048) · x + b     │
│                          │
│  Expert 0:  logit₀       │
│  Expert 1:  logit₁       │
│  Expert 2:  logit₂       │
│     ...                  │
│  Expert 127: logit₁₂₇    │
└──────┬───────────────────┘
       │
       ▼
┌──────────────┐
│   Logits     │
│   (128)      │
└──────────────┘
```

### Пример

```python
# Создание gate layer
hidden_size = 2048
num_experts = 128

gate = nn.Linear(2048, 128)

# Внутренние параметры:
print(gate.weight.shape)  # torch.Size([128, 2048])
print(gate.bias.shape)    # torch.Size([128])

# Forward pass
x = torch.randn(1, 2048)  # Один токен
logits = gate(x)
print(logits.shape)       # torch.Size([1, 128])
```

### Синтаксис `nn.Linear`

```python
torch.nn.Linear(
    in_features: int,      # Входная размерность
    out_features: int,     # Выходная размерность
    bias: bool = True      # Использовать ли bias (по умолчанию True)
)
```

---

## Строка 2: Инициализация весов

### Код
```python
self.gate.weight.data.normal_(0, 0.01)
```

### Что это делает?

Инициализирует матрицу весов `W` случайными значениями из **нормального распределения** N(μ=0, σ=0.01).

### Разбор синтаксиса

```python
self.gate.weight        # Доступ к тензору весов W
            .data       # Прямой доступ к данным (без autograd)
            .normal_(   # In-place операция нормального распределения
                0,      # mean (среднее) = 0
                0.01    # std (стандартное отклонение) = 0.01
            )
```

**⚠️ Важно**: Подчеркивание `_` означает **in-place операцию** (изменяет тензор на месте).

### Математика

```
W[i,j] ~ N(0, 0.01²)

Каждый элемент матрицы W независимо сэмплируется из:
  μ = 0      (центр распределения)
  σ = 0.01   (разброс значений)

Вероятностная плотность:
  p(x) = (1 / (0.01√(2π))) * exp(-(x-0)² / (2·0.01²))
```

### Визуализация распределения

```
      Frequency
         │
     ████│███           N(0, 0.01)
    ██████████
   ████████████         99.7% значений
  ██████████████        в диапазоне
 ████████████████       [-0.03, 0.03]
────┼────┼────┼────
  -0.03  0  0.03        Value
```

### Почему std=0.01?

| std | Проблема | Эффект |
|-----|----------|--------|
| **Слишком большое** (1.0) | Некоторые логиты >> 0, другие << 0 | После Softmax: сильный bias к определенным экспертам |
| **Слишком маленькое** (0.0001) | Все логиты ≈ 0 | После Softmax: все веса почти равны → медленное обучение |
| **0.01 ✅** | Небольшие различия | Оптимальный баланс: стабильность + возможность обучения |

### Пример значений

```python
# До инициализации (случайные значения из uniform distribution):
gate.weight[0, :5]
# tensor([ 0.4251,  0.1827, -0.2833,  0.3243, -0.1294])

# После normal_(0, 0.01):
gate.weight[0, :5]
# tensor([ 0.0042, -0.0087,  0.0013, -0.0052,  0.0098])
#         ^^^^^^   ^^^^^^   ^^^^^^   ^^^^^^   ^^^^^^
#       все значения малые, около нуля
```

### Альтернативные методы инициализации

```python
# Xavier/Glorot (хорошо для sigmoid/tanh)
nn.init.xavier_uniform_(self.gate.weight)

# Kaiming/He (хорошо для ReLU)
nn.init.kaiming_normal_(self.gate.weight)

# Равномерное распределение
self.gate.weight.data.uniform_(-0.01, 0.01)

# Константная инициализация (не рекомендуется)
self.gate.weight.data.fill_(0.01)
```

### Зачем нужен `.data`?

```python
# С .data (быстрее для инициализации)
self.gate.weight.data.normal_(0, 0.01)
# ✅ Изменяет данные напрямую, не отслеживает операцию в autograd

# Без .data
self.gate.weight.normal_(0, 0.01)
# ❌ RuntimeError: leaf variable has been used in an in-place operation
```

---

## Строка 3: Инициализация bias

### Код
```python
if self.gate.bias is not None:
    self.gate.bias.data.zero_()
```

### Что это делает?

Устанавливает все элементы вектора bias `b` в **ноль**, если bias существует.

### Разбор синтаксиса

```python
if self.gate.bias is not None:   # Проверка существования bias
    self.gate.bias.data          # Доступ к тензору bias
                  .zero_()       # In-place: заполняет нулями
```

### Математика

```
b = [0, 0, 0, ..., 0]  ∈ ℝ^128

b[i] = 0  для всех i ∈ {0, 1, ..., 127}
```

### Почему проверяем `is not None`?

```python
# По умолчанию bias создается:
gate1 = nn.Linear(2048, 128)               # bias=True (default)
print(gate1.bias)                          # tensor([...])

# Можно создать без bias:
gate2 = nn.Linear(2048, 128, bias=False)   # bias=False
print(gate2.bias)                          # None

# Попытка обращения к None вызовет ошибку:
# gate2.bias.data.zero_()                  # ❌ AttributeError
```

### Почему bias=0, а не случайные значения?

**Цель**: **Симметричное** начальное распределение по экспертам.

#### Сравнение

**С нулевым bias (правильно ✅):**
```python
logits = W · x + 0 = W · x

Для токена x = [1, -1, 0.5, ...]:
Expert 0: logit₀ = w₀ · x = 0.023
Expert 1: logit₁ = w₁ · x = -0.015
Expert 2: logit₂ = w₂ · x = 0.008
...

После Softmax все эксперты на равных
```

**С ненулевым bias (неправильно ❌):**
```python
logits = W · x + b

Если b = [1.0, 0, 0, ..., 0]:
Expert 0: logit₀ = w₀ · x + 1.0 = 1.023  ← Огромное преимущество!
Expert 1: logit₁ = w₁ · x + 0   = -0.015
Expert 2: logit₂ = w₂ · x + 0   = 0.008
...

Эксперт 0 всегда будет выбираться первым!
```

### Визуализация влияния bias

```
Распределение вероятностей после Softmax:

С bias=0 (равномерное):
Expert 0: ██████ 0.008
Expert 1: █████  0.007
Expert 2: ██████ 0.008
...
Expert 127: ████ 0.007

С bias[0]=1.0 (неравномерное):
Expert 0: ████████████████████████████ 0.987  ← Доминирует!
Expert 1: █ 0.001
Expert 2: █ 0.001
...
Expert 127: █ 0.001
```

### Альтернативы `.zero_()`

```python
# Вариант 1: zero_() (рекомендуется)
self.gate.bias.data.zero_()

# Вариант 2: fill_()
self.gate.bias.data.fill_(0)

# Вариант 3: присваивание
self.gate.bias.data = torch.zeros_like(self.gate.bias.data)
```

---

## Полная картина

### Что происходит при вызове gate

```python
# ═══════════════════════════════════════════════════════════════
# Шаг 1: Создание Router
# ═══════════════════════════════════════════════════════════════
router = MoERouter(hidden_size=2048, num_experts=128, top_k=8)

# После __init__:
#   gate.weight: (128, 2048) ~ N(0, 0.01)  ← малые случайные значения
#   gate.bias:   (128,) = [0, 0, ..., 0]   ← нули

# ═══════════════════════════════════════════════════════════════
# Шаг 2: Forward Pass
# ═══════════════════════════════════════════════════════════════
x = torch.randn(2, 10, 2048)  # batch=2, seq_len=10, hidden=2048
logits = router.gate(x)        # (2, 10, 128)

# ═══════════════════════════════════════════════════════════════
# Шаг 3: Детализация для одного токена
# ═══════════════════════════════════════════════════════════════
token = x[0, 0]  # Первый токен: (2048,)

# Вычисление логита для каждого эксперта:
for expert_i in range(128):
    # Скалярное произведение: вес эксперта · токен
    logit[expert_i] = gate.weight[expert_i] @ token + gate.bias[expert_i]
    #                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    #                   (2048,) · (2048,) = скаляр

    # Пример значений:
    # logit[0]   = 0.0234
    # logit[1]   = -0.0156
    # logit[2]   = 0.0087
    # ...
    # logit[127] = -0.0243

# Результат: 128 логитов (scores) для одного токена
```

### Пошаговая визуализация

```
┌─────────────────────────┐
│  Input Token            │
│  x = [0.5, -0.3, ...]   │
│  Shape: (2048,)         │
└────────────┬────────────┘
             │
             │  nn.Linear(2048, 128)
             │
             ▼
    ┌────────────────────┐
    │   Weight Matrix    │
    │   W: (128, 2048)   │
    │                    │
    │   Initialized:     │
    │   W[i,j] ~ N(0,0.01)│
    └────────┬───────────┘
             │
             │  Матричное умножение
             │  W · x
             ▼
    ┌────────────────────┐
    │   Weighted Sum     │
    │   (128,)           │
    └────────┬───────────┘
             │
             │  + bias
             │
             ▼
    ┌────────────────────┐
    │   Bias Vector      │
    │   b: (128,)        │
    │                    │
    │   Initialized:     │
    │   b[i] = 0         │
    └────────┬───────────┘
             │
             ▼
    ┌────────────────────┐
    │   Logits (Scores)  │
    │                    │
    │   [0.023, -0.015,  │
    │    0.008, ..., ]   │
    │                    │
    │   Shape: (128,)    │
    └────────────────────┘
             │
             │  Далее: Softmax → Top-K
             ▼
```

---

## Практический пример

### Полный код

```python
import torch
import torch.nn as nn

# ═══════════════════════════════════════════════════════════════
# 1. Создание gate layer
# ═══════════════════════════════════════════════════════════════
hidden_size = 4
num_experts = 3

gate = nn.Linear(hidden_size, num_experts)

print("="*60)
print("ДО ИНИЦИАЛИЗАЦИИ")
print("="*60)
print("\nWeight (веса):")
print(gate.weight.data)
print("\nBias (смещения):")
print(gate.bias.data)

# ═══════════════════════════════════════════════════════════════
# 2. Инициализация
# ═══════════════════════════════════════════════════════════════
gate.weight.data.normal_(0, 0.01)
if gate.bias is not None:
    gate.bias.data.zero_()

print("\n" + "="*60)
print("ПОСЛЕ ИНИЦИАЛИЗАЦИИ")
print("="*60)
print("\nWeight (веса):")
print(gate.weight.data)
print("  ↑ Маленькие значения около 0")
print("\nBias (смещения):")
print(gate.bias.data)
print("  ↑ Все нули")

# ═══════════════════════════════════════════════════════════════
# 3. Forward pass
# ═══════════════════════════════════════════════════════════════
print("\n" + "="*60)
print("FORWARD PASS")
print("="*60)

x = torch.randn(2, hidden_size)  # 2 токена, 4 features
print(f"\nInput (2 токена):")
print(x)

logits = gate(x)
print(f"\nLogits shape: {logits.shape}")
print("Logits (scores для каждого эксперта):")
print(logits)

# ═══════════════════════════════════════════════════════════════
# 4. Детальный расчет для первого токена
# ═══════════════════════════════════════════════════════════════
print("\n" + "="*60)
print("ДЕТАЛЬНЫЙ РАСЧЕТ ДЛЯ ТОКЕНА 0")
print("="*60)

token = x[0]  # Первый токен
print(f"\nТокен: {token}")

for expert_i in range(num_experts):
    weight_expert = gate.weight[expert_i]
    bias_expert = gate.bias[expert_i]

    # Скалярное произведение
    logit = torch.dot(weight_expert, token) + bias_expert

    print(f"\nЭксперт {expert_i}:")
    print(f"  w{expert_i} = {weight_expert}")
    print(f"  b{expert_i} = {bias_expert.item():.4f}")
    print(f"  logit{expert_i} = w{expert_i} · token + b{expert_i}")
    print(f"            = {logit.item():.4f}")

print("\n" + "="*60)
```

### Ожидаемый вывод

```
============================================================
ДО ИНИЦИАЛИЗАЦИИ
============================================================

Weight (веса):
tensor([[ 0.4251,  0.1827, -0.2833,  0.3243],
        [-0.1294,  0.4852,  0.2189, -0.3842],
        [ 0.2947, -0.1823,  0.4719, -0.2146]])

Bias (смещения):
tensor([-0.3894,  0.2731, -0.1982])

============================================================
ПОСЛЕ ИНИЦИАЛИЗАЦИИ
============================================================

Weight (веса):
tensor([[ 0.0042, -0.0087,  0.0013, -0.0052],
        [ 0.0098, -0.0034,  0.0067,  0.0021],
        [-0.0076,  0.0043, -0.0091,  0.0015]])
  ↑ Маленькие значения около 0

Bias (смещения):
tensor([0., 0., 0.])
  ↑ Все нули

============================================================
FORWARD PASS
============================================================

Input (2 токена):
tensor([[ 0.8347, -1.2531,  0.5234, -0.7829],
        [-0.3421,  1.5634, -0.8923,  0.2341]])

Logits shape: torch.Size([2, 3])
Logits (scores для каждого эксперта):
tensor([[-0.0123,  0.0087, -0.0045],
        [ 0.0056, -0.0098,  0.0034]])

============================================================
ДЕТАЛЬНЫЙ РАСЧЕТ ДЛЯ ТОКЕНА 0
============================================================

Токен: tensor([ 0.8347, -1.2531,  0.5234, -0.7829])

Эксперт 0:
  w0 = tensor([ 0.0042, -0.0087,  0.0013, -0.0052])
  b0 = 0.0000
  logit0 = w0 · token + b0
         = -0.0123

Эксперт 1:
  w1 = tensor([ 0.0098, -0.0034,  0.0067,  0.0021])
  b1 = 0.0000
  logit1 = w1 · token + b1
         = 0.0087

Эксперт 2:
  w2 = tensor([-0.0076,  0.0043, -0.0091,  0.0015])
  b2 = 0.0000
  logit2 = w2 · token + b2
         = -0.0045

============================================================
```

---

## Резюме

### Три ключевые строки

| Строка | Действие | Результат |
|--------|----------|-----------|
| `nn.Linear(hidden_size, num_experts)` | Создает слой | W: `(128, 2048)`, b: `(128,)` |
| `.weight.data.normal_(0, 0.01)` | Инициализирует веса | W[i,j] ~ N(0, 0.01) |
| `.bias.data.zero_()` | Обнуляет bias | b[i] = 0 |

### Почему именно так?

✅ **std=0.01**: Баланс между стабильностью и обучаемостью

✅ **bias=0**: Симметричное начальное распределение по экспертам

✅ **N(0, σ)**: Случайная инициализация ломает симметрию (разные эксперты учатся по-разному)

### Следующие шаги

После инициализации gate layer используется в методе `forward()`:

```python
# 1. Проекция в пространство экспертов
logits = self.gate(hidden_states)  # (batch, seq, num_experts)

# 2. Softmax для получения вероятностей
gating_scores = F.softmax(logits, dim=-1)

# 3. Top-K selection
routing_weights, selected_experts = torch.topk(gating_scores, self.top_k)

# 4. Renormalization
routing_weights = F.softmax(routing_weights, dim=-1)
```

---

## Полный процесс маршрутизации: От логитов к экспертам

### Пошаговый процесс

```
Токен → Gate Layer → Логиты → Softmax → Вероятности → Top-K → Выбранные эксперты
```

### Детальная визуализация

```
┌─────────────────────────────────────────────────────────────────┐
│ ШАГ 1: Gate Layer (Linear Projection)                           │
└─────────────────────────────────────────────────────────────────┘

Input Token: x = [0.5, -0.3, 0.8, ...]  (2048 features)
                        ↓
            gate(x) = W·x + b
                        ↓
Logits: [2.3, -1.5, 0.8, -0.3, 1.2, 0.5, -0.9, 1.8, ...]  (128 значений)
        ▲     ▲     ▲     ▲     ▲     ▲     ▲     ▲
        │     │     │     │     │     │     │     │
     Эксп0 Эксп1 Эксп2 Эксп3 Эксп4 Эксп5 Эксп6 Эксп7 ...

⚠️ Логиты могут быть любыми числами (отрицательными, > 1)

┌─────────────────────────────────────────────────────────────────┐
│ ШАГ 2: Softmax (Преобразование в вероятности)                   │
└─────────────────────────────────────────────────────────────────┘

Формула Softmax:
    p_i = exp(logit_i) / Σ exp(logit_j)

Логиты:      [2.3,  -1.5,  0.8,  -0.3,  1.2,  0.5,  -0.9,  1.8, ...]
                        ↓  Softmax  ↓
Вероятности: [0.123, 0.003, 0.027, 0.009, 0.040, 0.020, 0.005, 0.074, ...]
                                    │
                        Сумма всех = 1.0
                        Все значения в [0, 1]

┌─────────────────────────────────────────────────────────────────┐
│ ШАГ 3: Top-K Selection (K=8)                                    │
└─────────────────────────────────────────────────────────────────┘

Все 128 экспертов:
Эксп 0:  0.123  ◄── Входит в Top-8 (1-й по весу)
Эксп 1:  0.003
Эксп 2:  0.027
Эксп 3:  0.009
Эксп 4:  0.040  ◄── Входит в Top-8 (5-й по весу)
Эксп 5:  0.020
Эксп 6:  0.005
Эксп 7:  0.074  ◄── Входит в Top-8 (3-й по весу)
...
Эксп 42: 0.091  ◄── Входит в Top-8 (2-й по весу)
...
Эксп 99: 0.056  ◄── Входит в Top-8 (4-й по весу)
...

Выбранные эксперты (индексы): [0, 42, 7, 99, 4, 15, 88, 23]
Их веса (до ре-нормализации): [0.123, 0.091, 0.074, 0.056, 0.040, 0.035, 0.032, 0.028]
                                Сумма = 0.479 ≠ 1.0 ❌

┌─────────────────────────────────────────────────────────────────┐
│ ШАГ 4: Re-normalization (повторный Softmax только для Top-K)    │
└─────────────────────────────────────────────────────────────────┘

Веса до:  [0.123, 0.091, 0.074, 0.056, 0.040, 0.035, 0.032, 0.028]
                        ↓  Softmax  ↓
Веса после: [0.257, 0.190, 0.154, 0.117, 0.084, 0.073, 0.067, 0.058]
                                    │
                        Сумма = 1.0 ✅

┌─────────────────────────────────────────────────────────────────┐
│ ШАГ 5: Результат маршрутизации                                  │
└─────────────────────────────────────────────────────────────────┘

Для данного токена активируются 8 экспертов:

Эксперт 0:  вес = 0.257  (25.7% влияния)
Эксперт 42: вес = 0.190  (19.0% влияния)
Эксперт 7:  вес = 0.154  (15.4% влияния)
Эксперт 99: вес = 0.117  (11.7% влияния)
Эксперт 4:  вес = 0.084  (8.4% влияния)
Эксперт 15: вес = 0.073  (7.3% влияния)
Эксперт 88: вес = 0.067  (6.7% влияния)
Эксперт 23: вес = 0.058  (5.8% влияния)
                  ─────
           Сумма = 1.000  ✅

Остальные 120 экспертов не активируются (вес = 0)
```

### Почему не один эксперт, а Top-K?

**Традиционный подход (один эксперт):**
```python
best_expert = argmax(probabilities)  # Индекс с максимальной вероятностью
output = expert[best_expert](token)  # Только один эксперт обрабатывает токен
```

❌ **Проблемы:**
- Потеря информации от других экспертов
- Жесткое решение (hard routing)
- Нестабильность обучения

**MoE подход (Top-K экспертов):**
```python
top_k_experts, top_k_weights = topk(probabilities, k=8)
output = Σ (weight_i * expert_i(token))  # Взвешенная комбинация K экспертов
```

✅ **Преимущества:**
- Использование знаний нескольких экспертов
- Мягкое решение (soft routing) через взвешивание
- Более стабильное обучение
- Лучшая производительность модели

### Пример на реальных данных (K=8, N=128)

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

# ═══════════════════════════════════════════════════════════════
# Симуляция полного процесса
# ═══════════════════════════════════════════════════════════════

# Параметры
hidden_size = 2048
num_experts = 128
top_k = 8

# Gate layer (уже инициализированный)
gate = nn.Linear(hidden_size, num_experts)
gate.weight.data.normal_(0, 0.01)
gate.bias.data.zero_()

# Входной токен
token = torch.randn(1, hidden_size)  # (1, 2048)

# ШАГ 1: Получение логитов
logits = gate(token)  # (1, 128)
print(f"Логиты (первые 10): {logits[0, :10]}")
print(f"  Min: {logits.min():.4f}, Max: {logits.max():.4f}")
print(f"  Могут быть < 0 и > 1: ОК ✅\n")

# ШАГ 2: Softmax → вероятности
gating_scores = F.softmax(logits, dim=-1)  # (1, 128)
print(f"Вероятности (первые 10): {gating_scores[0, :10]}")
print(f"  Min: {gating_scores.min():.4f}, Max: {gating_scores.max():.4f}")
print(f"  Сумма: {gating_scores.sum():.4f}")
print(f"  Все в [0,1], сумма=1: ОК ✅\n")

# ШАГ 3: Top-K selection
top_k_weights, top_k_indices = torch.topk(gating_scores, k=top_k, dim=-1)
print(f"Top-{top_k} индексы: {top_k_indices[0]}")
print(f"Top-{top_k} веса (до ре-нормализации): {top_k_weights[0]}")
print(f"  Сумма весов: {top_k_weights.sum():.4f} ≠ 1.0 ❌\n")

# ШАГ 4: Re-normalization
routing_weights = F.softmax(top_k_weights, dim=-1)  # (1, 8)
print(f"Top-{top_k} веса (после ре-нормализации): {routing_weights[0]}")
print(f"  Сумма весов: {routing_weights.sum():.4f} = 1.0 ✅\n")

# ФИНАЛ: Результат маршрутизации
print("="*60)
print("РЕЗУЛЬТАТ МАРШРУТИЗАЦИИ")
print("="*60)
for i in range(top_k):
    expert_id = top_k_indices[0, i].item()
    weight = routing_weights[0, i].item()
    print(f"Эксперт {expert_id:3d}: вес = {weight:.4f} ({weight*100:5.2f}%)")
print(f"\nСумма весов: {routing_weights.sum():.4f}")
print(f"Активных экспертов: {top_k} из {num_experts}")
```

**Вывод:**
```
Логиты (первые 10): tensor([ 0.0234, -0.0156,  0.0087, -0.0243,  0.0165, ...])
  Min: -0.0389, Max: 0.0421
  Могут быть < 0 и > 1: ОК ✅

Вероятности (первые 10): tensor([0.0082, 0.0078, 0.0080, 0.0077, 0.0081, ...])
  Min: 0.0075, Max: 0.0084
  Сумма: 1.0000
  Все в [0,1], сумма=1: ОК ✅

Top-8 индексы: tensor([ 42,  15,   7, 123,  99,   4,  88,  23])
Top-8 веса (до ре-нормализации): tensor([0.0084, 0.0083, 0.0082, 0.0081, 0.0081, 0.0080, 0.0080, 0.0079])
  Сумма весов: 0.0650 ≠ 1.0 ❌

Top-8 веса (после ре-нормализации): tensor([0.1290, 0.1277, 0.1263, 0.1249, 0.1248, 0.1233, 0.1226, 0.1214])
  Сумма весов: 1.0000 = 1.0 ✅

============================================================
РЕЗУЛЬТАТ МАРШРУТИЗАЦИИ
============================================================
Эксперт  42: вес = 0.1290 (12.90%)
Эксперт  15: вес = 0.1277 (12.77%)
Эксперт   7: вес = 0.1263 (12.63%)
Эксперт 123: вес = 0.1249 (12.49%)
Эксперт  99: вес = 0.1248 (12.48%)
Эксперт   4: вес = 0.1233 (12.33%)
Эксперт  88: вес = 0.1226 (12.26%)
Эксперт  23: вес = 0.1214 (12.14%)

Сумма весов: 1.0000
Активных экспертов: 8 из 128
```

### Зачем нужна ре-нормализация?

**Математическое объяснение:**

После Top-K selection мы берем только K из N вероятностей. Их сумма < 1.0:

```
До Top-K:
p₁ + p₂ + p₃ + ... + p₁₂₈ = 1.0

После Top-K (выбрали 8 максимальных):
p_top1 + p_top2 + ... + p_top8 = 0.479 ≠ 1.0
```

Ре-нормализация гарантирует, что веса выбранных экспертов суммируются в 1.0:

```
routing_weight_i = exp(p_topi) / Σ exp(p_topj)
                                  j=1..K

routing_weight₁ + routing_weight₂ + ... + routing_weight₈ = 1.0 ✅
```

Это критически важно для корректного взвешенного комбинирования выходов экспертов.

---

**Дата создания**: 2025-10-01
**Автор**: AI Teacher для Qwen3 MoE Project

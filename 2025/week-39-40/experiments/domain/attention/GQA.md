# Grouped-Query Attention (GQA)

## Введение

Grouped-Query Attention (GQA) - это оптимизированная версия механизма Multi-Head Attention (MHA), предложенная для снижения вычислительных затрат и использования памяти при сохранении выразительной мощности. GQA используется в современных языковых моделях, включая Qwen3, LLaMA 3 и PaLM 2.

## Проблематика

Стандартный механизм Multi-Head Attention требует значительных вычислительных ресурсов и памяти, особенно для больших моделей. Для каждой головы внимания требуются отдельные проекции для запросов (queries), ключей (keys) и значений (values), что приводит к большому количеству параметров и вычислений.

## Решение: Grouped-Query Attention

GQA предлагает компромисс между эффективностью и выразительностью путем группировки запросов (queries) и совместного использования ключей (keys) и значений (values) между несколькими головами внимания.

### Ключевые особенности GQA:

1. **Группировка запросов**: вместо создания отдельных проекций для каждой головы внимания, запросы группируются, и каждая группа запросов используется несколькими головами внимания.

2. **Совместное использование ключей и значений**: ключи и значения используются совместно несколькими головами внимания, что значительно сокращает количество параметров и вычислений.

3. **Гибкость конфигурации**: количество групп запросов (num_query_groups) и количество голов внимания (num_attention_heads) могут настраиваться независимо, что позволяет найти оптимальный баланс между эффективностью и выразительностью.

## Математическая формула

Математически, GQA можно представить следующим образом:

$$
\mathrm{GQA}(Q, K, V) = \mathrm{Softmax}\left(\frac{QK^\top}{\sqrt{d_k}}\right)V
$$

где:
- $( Q )$ разделён на $( G )$ групп (меньше, чем количество голов для ключей и значений);
- $( K )$ и $( V )$ имеют $( H )$ голов ($( H \geq G )$);
- $ d_k $ — размерность векторов ключей (обычно совпадает с размерностью запросов $ d_q $, особенно в архитектурах типа Transformer);
- каждая группа запросов использует несколько голов ключей и значений.

## Сравнение с другими механизмами внимания

### Multi-Head Attention (MHA)

- **MHA**: каждая голова имеет свои проекции для query, key и value.
- **GQA**: запросы группируются, ключи и значения используются совместно.

### Multi-Query Attention (MQA)

- **MQA**: только один набор ключей и значений для всех голов запросов.
- **GQA**: несколько наборов ключей и значений, но меньше, чем голов запросов.

## Преимущества GQA

1. **Снижение вычислительных затрат**: меньше проекций для запросов, ключей и значений.
2. **Уменьшение использования памяти**: меньше параметров и промежуточных тензоров.
3. **Сохранение выразительной мощности**: по сравнению с MQA, GQA сохраняет большую часть выразительной мощности MHA.
4. **Улучшение масштабируемости**: позволяет эффективно масштабировать модели до больших размеров.
5. **Ускорение генерации**: особенно эффективно при автореггрессивной генерации, когда кэширование ключей и значений дает значительный выигрыш.

## Реализация в PyTorch

Основные компоненты реализации GQA:

```python
class GroupedQueryAttention(nn.Module):
    def __init__(
        self,
        hidden_size: int,
        num_query_groups: int,
        num_attention_heads: int,
        head_dim: Optional[int] = None,
        # ...
    ):
        # Проекции для query, key и value
        self.q_proj = nn.Linear(hidden_size, num_query_groups * head_dim)
        self.k_proj = nn.Linear(hidden_size, num_attention_heads * head_dim)
        self.v_proj = nn.Linear(hidden_size, num_attention_heads * head_dim)
        
        # Выходная проекция
        self.o_proj = nn.Linear(num_attention_heads * head_dim, hidden_size)
        
    def forward(self, hidden_states, ...):
        # Проекции
        query = self.q_proj(hidden_states)
        key = self.k_proj(hidden_states)
        value = self.v_proj(hidden_states)
        
        # Разделение на головы
        query = self._split_heads(query, self.num_query_groups)
        key = self._split_heads(key, self.num_attention_heads)
        value = self._split_heads(value, self.num_attention_heads)
        
        # Повторение query для соответствия количеству голов
        query = self._repeat_query(query, self.num_attention_heads // self.num_query_groups)
        
        # Вычисление внимания
        attention_scores = torch.matmul(query, key.transpose(-1, -2))
        attention_scores = attention_scores / math.sqrt(self.head_dim)
        
        # Применение softmax и получение весов внимания
        attention_weights = F.softmax(attention_scores, dim=-1)
        
        # Применение весов к значениям
        context = torch.matmul(attention_weights, value)
        
        # Объединение голов и применение выходной проекции
        context = self._combine_heads(context)
        output = self.o_proj(context)
        
        return output
```

## Интеграция с RoPE

Rotary Position Embedding (RoPE) часто используется вместе с GQA для добавления позиционной информации. RoPE применяется к запросам и ключам перед вычислением скалярного произведения.

```python
# Применение RoPE к query и key
if self.use_rope:
    query_rope, key_rope = self.rope(query, key, positions=position_ids)
    query = query_rope
    key = key_rope
```

## Кэширование ключей и значений

Для эффективной автореггрессивной генерации GQA поддерживает кэширование ключей и значений:

```python
# Использование кэшированных ключей и значений
if past_key_value is not None:
    past_key, past_value = past_key_value
    key = torch.cat([past_key, key], dim=-2)
    value = torch.cat([past_value, value], dim=-2)

# Подготовка нового past_key_value для следующего шага
if use_cache:
    present = (key, value)
else:
    present = None
```

## Применение в современных моделях

GQA успешно применяется в ряде современных языковых моделей:

1. **Qwen3**: использует GQA для повышения эффективности механизма внимания.
2. **LLaMA 3**: заменил MHA на GQA для улучшения производительности.
3. **PaLM 2**: использует GQA для снижения вычислительных затрат.
4. **Gemma**: применяет GQA для оптимизации памяти и вычислений.

## Рекомендации по настройке

1. **Соотношение num_query_groups и num_attention_heads**: обычно используется соотношение 1:2 или 1:4, например, 8 групп запросов и 16 или 32 головы внимания.

2. **Размерность головы (head_dim)**: обычно устанавливается как hidden_size / num_attention_heads, например, 64 или 128.

3. **Интеграция с другими оптимизациями**: GQA хорошо сочетается с другими оптимизациями, такими как Flash Attention и RoPE.

## Заключение

Grouped-Query Attention представляет собой эффективную оптимизацию механизма внимания, которая позволяет снизить вычислительные затраты и использование памяти при сохранении выразительной мощности. GQA особенно полезен для больших языковых моделей, где эффективность критически важна для масштабирования и практического применения.

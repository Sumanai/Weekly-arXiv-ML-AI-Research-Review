# RMSNorm — what the fuck?

## 1) Что такое «вектор активаций»

Начнем с того, что такое «вектор активаций»? В нейросетях **активации** — это численные выходы узлов (нейронов) слоя при прямом проходе. Для одного токена в трансформере это вектор скрытого состояния $x \in \mathbb{R}^d$ (последняя размерность тензора $(\text{batch}, \text{seq}, d)$).

* **Pre-activation (логиты)**: линейная комбинация входов слоя, например $Wx + b$.
* **Post-activation (активации в узком смысле)**: результат применения нелинейности (ReLU, GeLU, SiLU и т.п.) к логитам.

В разговорной практике «вектор активаций» могут называть как pre-, так и post-активации. Важно понимать контекст.

---

## 2) Где нормализуют в трансформерах

В современных LLM применяют **pre-norm**: нормализация идёт **перед** подблоками (Self-Attention и MLP) — то есть берут вектор после линейного преобразования, **но ещё до нелинейности** MLP.

Типичный путь сигнала для одного блока:

1. $x \;\rightarrow\; \text{RMSNorm}(x)$
2. Self-Attention → резидуальная сумма $h_1 = x + \text{Attn}(\text{RMSNorm}(x))$
3. $h_1 \;\rightarrow\; \text{RMSNorm}(h_1)$
4. MLP (линейный слой → **нелинейность** → линейный слой) → резидуальная сумма $y = h_1 + \text{MLP}(\text{RMSNorm}(h_1))$

**Вывод:** нормализация стабилизирует **масштаб входа** в подблок, а нелинейность применяется **после** неё (внутри MLP).

---

## 3) RMSNorm по сути

**Идея** 

RMSNorm — «облегчённая LayerNorm»: он **не вычитает среднее**, а только масштабирует вектор по его **RMS** (root mean square, корень из среднего квадрата компонент). Цель — контролировать масштаб активаций, не трогая их центр.

**Формула** 

Для $x=(x_1,\dots,x_d)$:

$$
\operatorname{RMSNorm}(x) 
= g \odot \frac{x}{\sqrt{\frac{1}{d}\sum_{i=1}^d x_i^2 + \varepsilon}},
$$

где: 

- $g\in\mathbb{R}^d$ — обучаемый масштабирующий вектор (часто без bias); [→ реализация в коде](rmsnorm.py#L68-L81)
- $\varepsilon$ — маленькая константа для численной стабильности; [→ реализация в коде](rmsnorm.py#L69)
- $\odot$ — поэлементное умножение. [→ реализация в коде](rmsnorm.py#L124)

**Интуитивно:** мы «приводим» длину (масштаб) вектора к разумному уровню, не смещая сам центр распределения.

**Реализация в коде:** [→ метод forward в rmsnorm.py](rmsnorm.py#L84-L128)

Основные шаги алгоритма:
1. [Вычисление квадратов элементов](rmsnorm.py#L115)
2. [Вычисление среднего квадратов](rmsnorm.py#L117)
3. [Вычисление RMS с добавлением epsilon](rmsnorm.py#L119)
4. [Нормализация входного тензора](rmsnorm.py#L121)
5. [Применение масштабирующего вектора](rmsnorm.py#L123-L126)

---

## 4) Сравнение RMSNorm vs LayerNorm

| Свойство                  | LayerNorm                          | RMSNorm                                  |
| ------------------------- | ---------------------------------- | ---------------------------------------- |
| Что делает                | Центрирует ($-\mu$) и масштабирует | Только масштабирует по RMS               |
| Чувствительность к сдвигу | Низкая (из-за вычитания среднего)  | Выше (сдвиг не удаляется)                |
| Параметры                 | $\gamma$ и $\beta$                 | $g$ (иногда добавляют bias, но редко)    |
| Стоимость вычислений      | Выше (нужны $\mu$ и $\sigma$)      | Ниже (только RMS)                        |
| FP16/FP8 устойчивость     | Чуть хуже                          | Лучше (меньше «ломких» операций)         |
| Где встречается           | BERT, GPT-2/3 и др.                | LLaMA-семейство и многие современные LLM |

**Почему RMSNorm работает в LLM:** в больших трансформерах ключевое — стабильный **масштаб** сигналов. Резидуалы и линейные слои компенсируют отсутствие центрирования, зато RMSNorm проще, быстрее и устойчивее при пониженной точности.

---

## 5) Короткий практический вывод

* **Что такое вектор активаций?** Числовой вектор выхода слоя на прямом проходе (обычно скрытое состояние токена).
* **Где нормализуем?** В pre-norm-архитектурах — **до** нелинейности соответствующего подблока (Attention/MLP).
* **Что делает RMSNorm?** Делит вектор на его RMS и масштабирует обучаемым $g$, не вычитая среднее. [→ полная реализация](rmsnorm.py)
* **Зачем это нужно?** Чтобы стабилизировать масштаб входа в подблоки, ускорить обучение и улучшить устойчивость в FP16/FP8.
* **Когда выбирать?** В современных LLM по умолчанию — RMSNorm, к LayerNorm есть смысл возвращаться, если критична инвариантность к сдвигу.

---

### Итог в одном предложении

**RMSNorm** — это нормализация «по длине» вектора активаций $x$ (без вычитания среднего), которую ставят **перед** подблоками трансформера, чтобы держать масштаб стабильным. Она проще и быстрее, чем LayerNorm, и потому стала стандартом в современных LLM.

### Связь с кодом

Полная реализация RMSNorm находится в файле [rmsnorm.py](rmsnorm.py):

- [Класс RMSNorm](rmsnorm.py#L9-L134) - полная реализация модуля
- [Метод __init__](rmsnorm.py#L48-L81) - инициализация параметров
- [Метод forward](rmsnorm.py#L84-L128) - основная логика нормализации
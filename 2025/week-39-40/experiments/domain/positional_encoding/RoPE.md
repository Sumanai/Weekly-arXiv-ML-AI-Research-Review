# RoPE — Rotary Position Embedding

## 1) Что такое позиционное кодирование

В трансформерах **позиционное кодирование** необходимо, потому что механизм внимания (attention) сам по себе не учитывает порядок токенов. Без позиционной информации модель воспринимала бы последовательность как набор (мешок) токенов.

* **Абсолютное позиционное кодирование**: добавляет вектор, зависящий только от абсолютной позиции токена (как в оригинальном Transformer).
* **Относительное позиционное кодирование**: кодирует относительные расстояния между токенами (как в RoPE, T5, и др.).

---

## 2) Что такое RoPE

**RoPE (Rotary Position Embedding)** — метод позиционного кодирования, основанный на вращении векторов в комплексной плоскости. Он был предложен в статье "RoFormer: Enhanced Transformer with Rotary Position Embedding" (Su et al., 2021).

### Ключевая идея

RoPE кодирует позицию путем вращения векторов query и key в пространстве эмбеддингов. Каждая пара соседних измерений $(d_{2m}, d_{2m+1})$ рассматривается как комплексное число, которое вращается на угол, пропорциональный позиции токена и частоте измерения.

### Математическая формула

Для позиции $m$ и размерности $d$:

$$
\Theta_m(d) = R_{\theta_{m,d}} \cdot x_d
$$

где:
- $R_{\theta_{m,d}}$ — матрица вращения
- $\theta_{m,d} = m \cdot \omega_d$ — угол вращения
- $\omega_d = 10000^{-2d/D}$ — частота для измерения $d$

В комплексной форме для пары измерений $(2i, 2i+1)$:

$$
\begin{pmatrix} x_{2i}' \\ x_{2i+1}' \end{pmatrix} = 
\begin{pmatrix} \cos(m\omega_i) & -\sin(m\omega_i) \\ \sin(m\omega_i) & \cos(m\omega_i) \end{pmatrix}
\begin{pmatrix} x_{2i} \\ x_{2i+1} \end{pmatrix}
$$

---

## 3) Преимущества RoPE

### Относительное позиционное кодирование

RoPE естественным образом кодирует **относительные** позиции. Скалярное произведение между двумя векторами с RoPE зависит только от расстояния между их позициями:

$$
\langle \Theta_m(q), \Theta_n(k) \rangle = f(q, k, m-n)
$$

### Инвариантность к сдвигу (Shift Invariance)

Паттерны внимания сохраняются при сдвиге всей последовательности, что делает модель более устойчивой к изменениям позиции.

### Экстраполяция на длинные контексты

RoPE показывает хорошую способность к экстраполяции на последовательности, длина которых превышает длину обучающих данных.

---

## 4) RoPE Scaling для длинных контекстов

Для улучшения экстраполяции на очень длинные контексты применяется **RoPE scaling**:

$$
\theta_{m,d} = \frac{m}{s} \cdot \omega_d
$$

где $s > 1$ — масштабирующий коэффициент (например, 2.0 для удвоения эффективной длины контекста).

### Интуитивно:
- Масштабирование замедляет вращение векторов с увеличением позиции
- Это позволяет модели лучше обрабатывать позиции, далеко выходящие за пределы обучающих данных

---

## 5) Реализация RoPE в PyTorch

### Основные шаги:

1. **Предварительное вычисление частот**:
   ```python
   freqs = base ** (-torch.arange(0, dim, 2) / dim)
   ```

2. **Вычисление углов для каждой позиции**:
   ```python
   angles = positions.unsqueeze(1) * freqs.unsqueeze(0)
   ```

3. **Вычисление sin и cos**:
   ```python
   cos = torch.cos(angles)
   sin = torch.sin(angles)
   ```

4. **Применение вращения**:
   ```python
   x_out[..., ::2] = x[..., ::2] * cos - x[..., 1::2] * sin
   x_out[..., 1::2] = x[..., 1::2] * cos + x[..., ::2] * sin
   ```

---

## 6) Сравнение с другими методами позиционного кодирования

| Метод | Тип | Преимущества | Недостатки |
|-------|-----|--------------|------------|
| Синусоидальный PE (Vanilla Transformer) | Абсолютный | Простота, не требует обучения | Ограниченная экстраполяция |
| Learned PE (BERT) | Абсолютный | Адаптивность к данным | Не экстраполирует за пределы обучения |
| ALiBi | Относительный | Хорошая экстраполяция | Применяется только к attention scores |
| T5 Relative Bias | Относительный | Учитывает направление | Требует обучаемых параметров |
| **RoPE** | Относительный | Инвариантность к сдвигу, экстраполяция | Требует четной размерности |

---

## 7) Применение в современных моделях

RoPE используется во многих современных языковых моделях:

- **LLaMA/LLaMA-2/LLaMA-3**
- **Qwen/Qwen2/Qwen3**
- **Mistral/Mixtral**
- **Falcon**
- **MPT**
- **Phi-2/Phi-3**

В большинстве моделей с длинным контекстом (32k+ токенов) применяется RoPE scaling для улучшения экстраполяции.

---

### Итог в одном предложении

**RoPE** — это метод позиционного кодирования, основанный на вращении векторов в комплексной плоскости, который естественным образом кодирует относительные позиции, обладает инвариантностью к сдвигу и хорошо экстраполируется на длинные контексты, что делает его стандартом де-факто в современных LLM.

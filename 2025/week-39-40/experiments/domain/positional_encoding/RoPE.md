# **RoPE — Rotary Position Embedding**  
*Позиционное кодирование через вращение в комплексной плоскости*

---

## **1. Комплексные числа — основа вращения**

### **Что такое комплексное число?**
Комплексное число — это пара действительных чисел $(a,b)$, записываемая как  
$z = a + ib$, где $i^2 = -1$.

- $\Re z = a$ — действительная часть  
- $\Im z = b$ — мнимая часть  
- Геометрически: точка или вектор на плоскости с координатами $(a, b)$

---

### **Полярная форма**
Любое $z \neq 0$ можно представить как:  
$z = r(\cos\varphi + i\sin\varphi) = r\,e^{i\varphi}$,  
где:
- $r = |z| = \sqrt{a^2 + b^2}$ — модуль  
- $\varphi = \arg z$ — аргумент (угол с осью $x$)

---

### **Почему это важно?**
Умножение в полярной форме:  
$(r_1 e^{i\varphi_1})(r_2 e^{i\varphi_2}) = (r_1 r_2)\, e^{i(\varphi_1+\varphi_2)}$.

→ Модули умножаются, углы **складываются**.  
→ Умножение на $e^{i\theta}$ — это **чистый поворот** на угол $\theta$ без изменения длины.

---

### **Связь с матрицей поворота**
Вектор $(x, y)$ соответствует комплексному числу $z = x + iy$.  
Умножение $z \cdot e^{i\theta}$ эквивалентно применению матрицы поворота:  
$$
R(\theta)=\begin{pmatrix}\cos\theta&-\sin\theta\\ \sin\theta&\cos\theta\end{pmatrix}.
$$

> **Эта эквивалентность — ключ к пониманию RoPE.**

---

## **2. Позиционное кодирование в трансформерах**

### **Проблема**
Механизм внимания **не учитывает порядок токенов** — без позиционной информации последовательность воспринимается как «мешок слов».

---

### **Два подхода**

#### **Абсолютное позиционное кодирование**
Добавляется фиксированный вектор, зависящий **только от позиции токена**.

**Пример (d=4):**
```text
Эмбеддинги токенов X:
[0.2, 0.5, 0.1, 0.7]   # "Я"
[0.3, 0.6, 0.0, 0.8]   # "люблю"
[0.9, 0.1, 0.4, 0.3]   # "машинное"
[0.5, 0.2, 0.9, 0.6]   # "обучение"

Абсолютные позиции P:
[1.0, 0.0, 0.0, 0.0]   # позиция 1
[0.0, 1.0, 0.0, 0.0]   # позиция 2
[0.0, 0.0, 1.0, 0.0]   # позиция 3
[0.0, 0.0, 0.0, 1.0]   # позиция 4

Итог (X + P):
[1.2, 0.5, 0.1, 0.7]
[0.3, 1.6, 0.0, 0.8]
[0.9, 0.1, 1.4, 0.3]
[0.5, 0.2, 0.9, 1.6]
```

→ Каждый токен получает **уникальный «адрес»**.

---

#### **Относительное позиционное кодирование**
Кодирует **расстояние между токенами**, а не их абсолютные позиции.

**Матрица разностей позиций $\Delta$:**
```text
[ 0, -1, -2, -3]
[ 1,  0, -1, -2]
[ 2,  1,  0, -1]
[ 3,  2,  1,  0]
```

- Элемент $(i, j)$: насколько токен $j$ дальше от токена $i$  
- Примеры:
  - "Я" → "люблю": $-1$  
  - "Я" → "обучение": $-3$

→ Модель «видит» **локальные и глобальные отношения**.

---

## **3. Что такое RoPE?**

**RoPE** (*Rotary Position Embedding*) — метод позиционного кодирования, основанный на **вращении векторов в комплексной плоскости** (Su et al., 2021, *RoFormer*).

---

### **Зачем нужны разные частоты?**

Если все измерения вращаются с одинаковой скоростью → через некоторое время векторы совпадут → **потеря уникальности расстояний**.

**Решение:** использовать **набор частот**:
- **Быстрые частоты** → различают близкие токены  
- **Медленные частоты** → различают далёкие токены  
→ Вместе дают **уникальный «отпечаток»** для каждого расстояния.

---

### **Как работает RoPE?**

#### **1. Токен → вектор**
После эмбеддингов и линейных преобразований:
- $q_p = x_p W_Q$ — query-вектор токена на позиции $p$  
- $k_p = x_p W_K$ — key-вектор токена на позиции $p$  
- Оба имеют размерность $d_{\text{head}}$

> **RoPE применяется именно к $q_p$ и $k_p$**, а не к исходным эмбеддингам.

---

#### **2. Группировка по парам**
Первые $d_{\text{rot}}$ компонент разбиваются на пары:  
$(x_0, x_1), (x_2, x_3), \dots$  
→ Каждая пара = «стрелка» на плоскости = комплексное число.

---

#### **3. Частоты $\omega_i$**
Для каждой пары задаётся своя частота:  
$\omega_i = b^{-\frac{2i}{d_{\text{rot}}}}$, где $i = 0, 1, \dots, \frac{d_{\text{rot}}}{2} - 1$.

- Первая пара — быстро вращается  
- Последняя — медленно  
→ Геометрическая прогрессия скоростей.

---

#### **4. Угол для позиции $p$**
$\theta_{p,i} = p \cdot \omega_i$  
→ Угол накапливается с каждой позицией.

---

#### **5. Взаимодействие**
- По **токенам**: угол растёт линейно с позицией  
- По **измерениям**: разные частоты → разные масштабы восприятия

---

### **Математическая формула**

Для позиции $m$ и размерности $d$:  
$\Theta_m(d) = R_{\theta_{m,d}} \cdot x_d$,  
где:
- $R_{\theta_{m,d}}$ — матрица вращения  
- $\theta_{m,d} = m \cdot \omega_d$  
- $\omega_d = 10000^{-2d/D}$

**Для пары $(2i, 2i+1)$ результат вращения:**
- Для четных индексов (действительная часть): $x_{2i}' = x_{2i}\cos(\theta) - x_{2i+1}\sin(\theta)$ (вычитание)
- Для нечетных индексов (мнимая часть): $x_{2i+1}' = x_{2i}\sin(\theta) + x_{2i+1}\cos(\theta)$ (сложение)

---

## **4. Преимущества RoPE**

| Свойство | Объяснение |
|--------|-----------|
| **Относительное кодирование** | Скалярное произведение $\langle \Theta_m(q), \Theta_n(k) \rangle = f(q, k, m-n)$ зависит **только от разности позиций** |
| **Инвариантность к сдвигу** | Паттерны внимания не меняются при сдвиге всей последовательности |
| **Экстраполяция** | Хорошо работает на последовательностях **длиннее обучающих данных** |

---

## **5. RoPE Scaling для длинных контекстов**

Для улучшения экстраполяции:  
$\theta_{m,d} = \frac{m}{s} \cdot \omega_d$, где $s > 1$.

- **Интуиция**: замедление вращения → векторы не «накладываются» на больших дистанциях  
- Пример: $s = 2.0$ → эффективная длина контекста удваивается

---

## **6. Реализация RoPE в PyTorch**

### **Основные шаги**
```python
# 1. Частоты
freqs = base ** (-torch.arange(0, dim, 2) / dim)

# 2. Углы для позиций
angles = positions.unsqueeze(1) * freqs.unsqueeze(0)

# 3. sin и cos
cos = torch.cos(angles)
sin = torch.sin(angles)

# 4. Применение вращения
x_out[..., ::2] = x[..., ::2] * cos - x[..., 1::2] * sin
x_out[..., 1::2] = x[..., 1::2] * cos + x[..., ::2] * sin
```

---

### **Почему именно такие формулы?**

#### **1. Знаки вращения**
Каждая пара $(x_{2i}, x_{2i+1})$ интерпретируется как комплексное число $x_{2i} + i x_{2i+1}$.  
Умножение на $e^{i\theta} = \cos\theta + i\sin\theta$ даёт:

- Для четных индексов (действительная часть): $x_{2i}' = x_{2i}\cos(\theta) - x_{2i+1}\sin(\theta)$ (вычитание)
- Для нечетных индексов (мнимая часть): $x_{2i+1}' = x_{2i}\sin(\theta) + x_{2i+1}\cos(\theta)$ (сложение)

→ Это прямое следствие умножения комплексных чисел.

#### **2. Инверсия для ключей**
Чтобы скалярное произведение зависело от $m - n$, нужно:
- Query: вращать как $e^{+i\theta}$ → $\cos(\theta) + i\sin(\theta)$  
- Key: вращать как $e^{-i\theta}$ → $\cos(\theta) - i\sin(\theta)$

→ Достигается **сменой знака у $\sin$** при применении RoPE к ключам.

→ Это обеспечивает **относительное позиционное кодирование «из коробки»**.

---

## **7. Сравнение с другими методами**

| Метод | Тип | Преимущества | Недостатки |
|-------|-----|--------------|------------|
| Синусоидальный PE (Vanilla Transformer) | Абсолютный | Простота, не обучаемый | Плохая экстраполяция |
| Learned PE (BERT) | Абсолютный | Адаптивность | Не экстраполирует |
| ALiBi | Относительный | Отличная экстраполяция | Применяется только к attention scores |
| T5 Relative Bias | Относительный | Учитывает направление | Требует обучаемых параметров |
| **RoPE** | Относительный | Инвариантность к сдвигу, экстраполяция, теоретически обоснован | Требует чётной размерности |

---

## **8. Где используется RoPE?**

RoPE — **стандарт де-факто** в современных LLM:

- **LLaMA / LLaMA-2 / LLaMA-3**  
- **Qwen / Qwen2 / Qwen3**  
- **Mistral / Mixtral**  
- **Falcon**  
- **MPT**  
- **Phi-2 / Phi-3**

> В моделях с **длинным контекстом** (32k+ токенов) почти всегда применяется **RoPE scaling**.

---

## **Итог в одном предложении**

> **RoPE** — это метод позиционного кодирования, основанный на вращении векторов в комплексной плоскости, который естественным образом кодирует относительные позиции, обладает инвариантностью к сдвигу и хорошо экстраполируется на длинные контексты, что делает его стандартом де-факто в современных LLM.
# RoPE — Rotary Position Embedding

## 1) Что такое позиционное кодирование

В трансформерах **позиционное кодирование** необходимо, потому что механизм внимания (attention) сам по себе не учитывает порядок токенов. Без позиционной информации модель воспринимала бы последовательность как набор (мешок) токенов.

* **Абсолютное позиционное кодирование**: добавляет вектор, зависящий только от **абсолютной позиции токена** в предложении (как в оригинальном Transformer).
  Например, для последовательности из 4 токенов матрица эмбеддингов после добавления абсолютных позиций может выглядеть так:

  ```
  Эмбеддинги токенов (размерность d=4):
  X = [
    [0.2, 0.5, 0.1, 0.7],   # "Я"
    [0.3, 0.6, 0.0, 0.8],   # "люблю"
    [0.9, 0.1, 0.4, 0.3],   # "машинное"
    [0.5, 0.2, 0.9, 0.6]    # "обучение"
  ]

  Абсолютные позиции (P):
  [
    [1.0, 0.0, 0.0, 0.0],   # позиция 1
    [0.0, 1.0, 0.0, 0.0],   # позиция 2
    [0.0, 0.0, 1.0, 0.0],   # позиция 3
    [0.0, 0.0, 0.0, 1.0]    # позиция 4
  ]

  Итоговые векторы (X + P):
  [
    [1.2, 0.5, 0.1, 0.7],
    [0.3, 1.6, 0.0, 0.8],
    [0.9, 0.1, 1.4, 0.3],
    [0.5, 0.2, 0.9, 1.6]
  ]
  ```

  Здесь каждый токен получил "адрес" в последовательности, связанный только с его позицией.

* **Относительное позиционное кодирование**: кодирует **расстояние между токенами**, а не их фиксированное место (как в RoPE, T5 и др.).
  Для той же последовательности матрица относительных расстояний будет:

  ```
  Δ (разности позиций):
  [
    [ 0, -1, -2, -3],
    [ 1,  0, -1, -2],
    [ 2,  1,  0, -1],
    [ 3,  2,  1,  0]
  ]
  ```

  Здесь элемент (i, j) показывает, насколько **токен j дальше от токена i**.
  Например:

  * "Я" и "люблю" → расстояние = -1 (один шаг вперёд)
  * "люблю" и "машинное" → расстояние = -1
  * "Я" и "обучение" → расстояние = -3

  Такая матрица позволяет attention-функции зависеть от **разности позиций**, что делает модель более гибкой и устойчивой к сдвигам текста.

---

Таким образом:

* Абсолютное кодирование фиксирует каждый токен в его **уникальной позиции**.
* Относительное кодирование позволяет модели "видеть", **кто стоит рядом и на каком расстоянии**, что ближе к человеческому восприятию последовательности.

---

## 2) Что такое RoPE

**RoPE (Rotary Position Embedding)** — метод позиционного кодирования, основанный на вращении векторов в комплексной плоскости. Он был предложен в статье "RoFormer: Enhanced Transformer with Rotary Position Embedding" (Su et al., 2021).

### Ключевая идея

RoPE кодирует позицию путем вращения векторов query и key в пространстве эмбеддингов. Каждая пара соседних измерений $(d_{2m}, d_{2m+1})$ рассматривается как комплексное число, которое вращается на угол, пропорциональный позиции токена и частоте измерения.

### Математическая формула

Для позиции $m$ и размерности $d$:

$$
\Theta_m(d) = R_{\theta_{m,d}} \cdot x_d
$$

где:
- $R_{\theta_{m,d}}$ — матрица вращения
- $\theta_{m,d} = m \cdot \omega_d$ — угол вращения
- $\omega_d = 10000^{-2d/D}$ — частота для измерения $d$

В комплексной форме для пары измерений $(2i, 2i+1)$:

$$
\begin{pmatrix} x_{2i}' \\ x_{2i+1}' \end{pmatrix} = 
\begin{pmatrix} \cos(m\omega_i) & -\sin(m\omega_i) \\ \sin(m\omega_i) & \cos(m\omega_i) \end{pmatrix}
\begin{pmatrix} x_{2i} \\ x_{2i+1} \end{pmatrix}
$$

---

## 3) Преимущества RoPE

### Относительное позиционное кодирование

RoPE естественным образом кодирует **относительные** позиции. Скалярное произведение между двумя векторами с RoPE зависит только от расстояния между их позициями:

$$
\langle \Theta_m(q), \Theta_n(k) \rangle = f(q, k, m-n)
$$

### Инвариантность к сдвигу (Shift Invariance)

Паттерны внимания сохраняются при сдвиге всей последовательности, что делает модель более устойчивой к изменениям позиции.

### Экстраполяция на длинные контексты

RoPE показывает хорошую способность к экстраполяции на последовательности, длина которых превышает длину обучающих данных.

---

## 4) RoPE Scaling для длинных контекстов

Для улучшения экстраполяции на очень длинные контексты применяется **RoPE scaling**:

$$
\theta_{m,d} = \frac{m}{s} \cdot \omega_d
$$

где $s > 1$ — масштабирующий коэффициент (например, 2.0 для удвоения эффективной длины контекста).

### Интуитивно:
- Масштабирование замедляет вращение векторов с увеличением позиции
- Это позволяет модели лучше обрабатывать позиции, далеко выходящие за пределы обучающих данных

---

## 5) Реализация RoPE в PyTorch

### Основные шаги:

1. **Предварительное вычисление частот**:
   ```python
   freqs = base ** (-torch.arange(0, dim, 2) / dim)
   ```

2. **Вычисление углов для каждой позиции**:
   ```python
   angles = positions.unsqueeze(1) * freqs.unsqueeze(0)
   ```

3. **Вычисление sin и cos**:
   ```python
   cos = torch.cos(angles)
   sin = torch.sin(angles)
   ```

4. **Применение вращения**:
   ```python
   x_out[..., ::2] = x[..., ::2] * cos - x[..., 1::2] * sin
   x_out[..., 1::2] = x[..., 1::2] * cos + x[..., ::2] * sin
   ```

---

## 6) Сравнение с другими методами позиционного кодирования

| Метод | Тип | Преимущества | Недостатки |
|-------|-----|--------------|------------|
| Синусоидальный PE (Vanilla Transformer) | Абсолютный | Простота, не требует обучения | Ограниченная экстраполяция |
| Learned PE (BERT) | Абсолютный | Адаптивность к данным | Не экстраполирует за пределы обучения |
| ALiBi | Относительный | Хорошая экстраполяция | Применяется только к attention scores |
| T5 Relative Bias | Относительный | Учитывает направление | Требует обучаемых параметров |
| **RoPE** | Относительный | Инвариантность к сдвигу, экстраполяция | Требует четной размерности |

---

## 7) Применение в современных моделях

RoPE используется во многих современных языковых моделях:

- **LLaMA/LLaMA-2/LLaMA-3**
- **Qwen/Qwen2/Qwen3**
- **Mistral/Mixtral**
- **Falcon**
- **MPT**
- **Phi-2/Phi-3**

В большинстве моделей с длинным контекстом (32k+ токенов) применяется RoPE scaling для улучшения экстраполяции.

---

### Итог в одном предложении

**RoPE** — это метод позиционного кодирования, основанный на вращении векторов в комплексной плоскости, который естественным образом кодирует относительные позиции, обладает инвариантностью к сдвигу и хорошо экстраполируется на длинные контексты, что делает его стандартом де-факто в современных LLM.

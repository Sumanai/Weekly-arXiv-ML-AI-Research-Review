# От GPT-2 к gpt-oss: анализ достижений архитектуры

> И как они выглядят на фоне Qwen3

5 августа, 2025 года OpenAI выпустила новые модели LLM с открытым весом: gpt-oss-120b и gpt-oss-20b — первые полностью открытые модели с момента выхода GPT-2 в 2019 году. И да, благодаря некоторым умным оптимизациям, их можно запускать локально (но об этом чуть позже).

Это первый раз с момента выпуска GPT-2, когда OpenAI делится крупной полностью открытой моделью. Ранние модели GPT показали, как масштабируется архитектура трансформеров. Затем выпуск ChatGPT в 2022 году сделал эти модели мейнстримом, продемонстрировав их практическую пользу для задач письма, получения знаний (а позже и программирования). Теперь же компания поделилась долгожданными весами модели, и архитектура содержит несколько интересных деталей.

Я провёл последние несколько дней, изучая код и технические отчёты, чтобы обобщить самые интересные подробности. (Спустя всего несколько дней после этого OpenAI также анонсировала GPT-5 — я кратко затрону её в контексте моделей gpt-oss в конце статьи.)

Ниже — краткий обзор того, о чём пойдёт речь в статье. Для удобной навигации рекомендую использовать оглавление слева на странице статьи.

- Сравнение архитектуры моделей с GPT-2  
- Оптимизация MXFP4, позволяющая разместить модели gpt-oss на одной видеокарте  
- Компромиссы между шириной и глубиной (gpt-oss vs Qwen3)  
- Внимание, смещения и «поглотители» (attention bias and sinks)  
- Бенчмарки и сравнение с GPT-5  

Надеюсь, вы найдёте эту статью полезной!

## 1. Обзор архитектуры модели

Прежде чем подробно обсуждать архитектуру, начнём с обзора двух моделей — `gpt-oss-20b` и `gpt-oss-120b`, показанных на Рисунке 1 ниже.

![Рисунок 1: Две модели gpt-oss рядом.](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-39-40/assets/Figure-01.png)

Если вы раньше видели схемы современных LLM или читали мою предыдущую статью «Большое сравнение архитектур», вы можете заметить, что на первый взгляд здесь нет ничего принципиально нового или необычного.

Это неудивительно: ведущие разработчики LLM, как правило, используют одну и ту же базовую архитектуру, внося лишь небольшие доработки. Это моё личное предположение, но, думаю, причины в следующем:

- Между лабораториями происходит значительная ротация сотрудников.
- Мы до сих пор не нашли ничего лучше архитектуры трансформера. Хотя существуют state space models и модели диффузии текста, насколько мне известно, никто не доказал, что они работают так же хорошо, как трансформеры, на данном масштабе. (Большинство сравнений, которые я нашёл, сосредоточены только на результатах бенчмарков. До сих пор неясно, насколько хорошо эти модели справляются с реальными многоходовыми задачами письма и программирования. На момент написания статьи самая высокорейтинговая неполностью-трансформерная модель на LM Arena — Jamba, гибрид трансформера и state space модели, занимает 96-е место. *ПРИМЕЧАНИЕ: мне любезно указали, что существует более высокорейтинговая гибридная модель — Hunyuan-TurboS на 22-м месте.*)
- Большая часть улучшений, скорее всего, достигается за счёт данных и тонкой настройки алгоритмов, а не за счёт кардинальных изменений архитектуры.

Тем не менее, в их выборе дизайна есть множество интересных аспектов. Некоторые из них показаны на рисунке выше (другие — нет, но мы обсудим их позже). В оставшейся части статьи я поочерёдно выделю эти особенности и сравню их с другими архитектурами.

Также хочу отметить, что я никоим образом не связан с OpenAI. Моя информация основана исключительно на изучении опубликованного кода модели и технических отчётов. Если вы хотите узнать, как использовать эти модели локально, лучшее место для начала — официальные страницы моделей OpenAI на Hugging Face Hub:

- https://huggingface.co/openai/gpt-oss-20b
- https://huggingface.co/openai/gpt-oss-120b

Модель на 20 млрд параметров (`gpt-oss-20b`) может работать на потребительской видеокарте с 16 ГБ ОЗУ. Модель на 120 млрд параметров (`gpt-oss-120b`) может работать на одной карте NVIDIA H100 с 80 ГБ ОЗУ или на более новом оборудовании. Я вернусь к этому позже, поскольку есть несколько важных оговорок.

---
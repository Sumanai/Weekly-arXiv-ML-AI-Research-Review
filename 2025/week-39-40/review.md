# От GPT-2 к gpt-oss: анализ достижений архитектуры

> И как они выглядят на фоне Qwen3

5 августа, 2025 года OpenAI выпустила новые модели LLM с открытым весом: gpt-oss-120b и gpt-oss-20b — первые полностью открытые модели с момента выхода GPT-2 в 2019 году. И да, благодаря некоторым умным оптимизациям, их можно запускать локально (но об этом чуть позже).

Это первый раз с момента выпуска GPT-2, когда OpenAI делится крупной полностью открытой моделью. Ранние модели GPT показали, как масштабируется архитектура трансформеров. Затем выпуск ChatGPT в 2022 году сделал эти модели мейнстримом, продемонстрировав их практическую пользу для задач письма, получения знаний (а позже и программирования). Теперь же компания поделилась долгожданными весами модели, и архитектура содержит несколько интересных деталей.

Я провёл последние несколько дней, изучая код и технические отчёты, чтобы обобщить самые интересные подробности. (Спустя всего несколько дней после этого OpenAI также анонсировала GPT-5 — я кратко затрону её в контексте моделей gpt-oss в конце статьи.)

Ниже — краткий обзор того, о чём пойдёт речь в статье. Для удобной навигации рекомендую использовать оглавление слева на странице статьи.

- Сравнение архитектуры моделей с GPT-2  
- Оптимизация MXFP4, позволяющая разместить модели gpt-oss на одной видеокарте  
- Компромиссы между шириной и глубиной (gpt-oss vs Qwen3)  
- Внимание, смещения и «поглотители» (attention bias and sinks)  
- Бенчмарки и сравнение с GPT-5  

Надеюсь, вы найдёте эту статью полезной!

## 1. Обзор архитектуры модели

Прежде чем подробно обсуждать архитектуру, начнём с обзора двух моделей — `gpt-oss-20b` и `gpt-oss-120b`, показанных на Рисунке 1 ниже.

![Рисунок 1: Две модели gpt-oss рядом.](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-39-40/assets/Figure-01.png)

Если вы раньше видели схемы современных LLM или читали мою предыдущую статью «Большое сравнение архитектур», вы можете заметить, что на первый взгляд здесь нет ничего принципиально нового или необычного.

Это неудивительно: ведущие разработчики LLM, как правило, используют одну и ту же базовую архитектуру, внося лишь небольшие доработки. Это моё личное предположение, но, думаю, причины в следующем:

- Между лабораториями происходит значительная ротация сотрудников.
- Мы до сих пор не нашли ничего лучше архитектуры трансформера. Хотя существуют state space models и модели диффузии текста, насколько мне известно, никто не доказал, что они работают так же хорошо, как трансформеры, на данном масштабе. (Большинство сравнений, которые я нашёл, сосредоточены только на результатах бенчмарков. До сих пор неясно, насколько хорошо эти модели справляются с реальными многоходовыми задачами письма и программирования. На момент написания статьи самая высокорейтинговая неполностью-трансформерная модель на LM Arena — Jamba, гибрид трансформера и state space модели, занимает 96-е место. *ПРИМЕЧАНИЕ: мне любезно указали, что существует более высокорейтинговая гибридная модель — Hunyuan-TurboS на 22-м месте.*)
- Большая часть улучшений, скорее всего, достигается за счёт данных и тонкой настройки алгоритмов, а не за счёт кардинальных изменений архитектуры.

Тем не менее, в их выборе дизайна есть множество интересных аспектов. Некоторые из них показаны на рисунке выше (другие — нет, но мы обсудим их позже). В оставшейся части статьи я поочерёдно выделю эти особенности и сравню их с другими архитектурами.

Также хочу отметить, что я никоим образом не связан с OpenAI. Моя информация основана исключительно на изучении опубликованного кода модели и технических отчётов. Если вы хотите узнать, как использовать эти модели локально, лучшее место для начала — официальные страницы моделей OpenAI на Hugging Face Hub:

- https://huggingface.co/openai/gpt-oss-20b
- https://huggingface.co/openai/gpt-oss-120b

Модель на 20 млрд параметров (`gpt-oss-20b`) может работать на потребительской видеокарте с 16 ГБ ОЗУ. Модель на 120 млрд параметров (`gpt-oss-120b`) может работать на одной карте NVIDIA H100 с 80 ГБ ОЗУ или на более новом оборудовании. Я вернусь к этому позже, поскольку есть несколько важных оговорок.

---

## 2. Наследие GPT-2

Прежде чем углубиться в сравнение gpt-oss с более современными архитектурами, давайте совершим путешествие во времени и сравним ее бок о бок с GPT-2 (Рисунок 2), чтобы наглядно увидеть, какой путь был пройден.

![Рисунок 2: Сравнение архитектур gpt-oss-20b и GPT-2 XL 1.5B.](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-39-40/assets/Figure-02.jpg)

И gpt-oss, и GPT-2 — это LLM, использующие только декодер и построенные на архитектуре трансформера, представленной в знаменитой статье «Attention Is All You Need» (2017). За прошедшие годы многие детали этой архитектуры эволюционировали.

Впрочем, эти изменения не являются уникальными для gpt-oss. Как мы увидим далее, они встречаются во многих других современных языковых моделях. Поскольку я уже подробно обсуждал многие из этих аспектов в предыдущей статье «Большое сравнение архитектур», я постараюсь быть краткими и сосредоточиться на ключевых моментах.

### 2.1 Отказ от Dropout

Dropout (2012) — это классический метод предотвращения переобучения, который случайным образом «отключает» (то есть обнуляет) часть активаций слоя или оценок внимания (Рисунок 3) во время обучения. Однако в современных больших языковых моделях dropout используется крайне редко, и большинство моделей, вышедших после GPT-2, от него отказались.

![Рисунок 3: Иллюстрация применения dropout к матрице оценок внимания.](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-39-40/assets/Figure-03.png)

Можно предположить, что изначально dropout был использован в GPT-2 как наследие оригинальной архитектуры трансформера. Исследователи, скорее всего, заметили, что он не дает реального улучшения производительности LLM (я наблюдал то же самое в своих небольших экспериментах по воспроизведению GPT-2). Это связано с тем, что LLM обычно обучаются всего за одну эпоху на огромных наборах данных, в отличие от режимов обучения в сотни эпох, для которых dropout изначально был создан. Поскольку LLM видят каждый токен только один раз за всё обучение, риск переобучения невелик.

Что интересно, хотя dropout много лет игнорировался при проектировании архитектур LLM, я нашел исследовательскую статью 2025 года с экспериментами на относительно небольших моделях (Pythia 1.4B), которая подтверждает, что в условиях обучения в одну эпоху dropout приводит к ухудшению итогового качества модели.

### 2.2 RoPE заменяет абсолютные позиционные эмбеддинги

В трансформерных LLM позиционное кодирование необходимо из-за механизма внимания. По умолчанию attention рассматривает входные токены так, как если бы они не имели порядка. В оригинальной архитектуре GPT эту проблему решали абсолютные позиционные эмбеддинги: к вектору токена добавлялся изученный вектор, соответствующий его позиции в последовательности (Рисунок 4).

![Рисунок 4: Иллюстрация абсолютных позиционных эмбеддингов.](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-39-40/assets/Figure-04.jpg)

RoPE (Rotary Position Embedding) предложила другой подход: вместо добавления позиционной информации в виде отдельных векторов, она кодирует позицию путем вращения векторов запроса и ключа, которое зависит от позиции каждого токена. (Идея RoPE элегантна, но ее объяснение — тема сложная, которую я планирую подробно разобрать отдельно.)

Впервые представленные в 2021 году, RoPE получили широкое распространение с выходом оригинальной модели Llama в 2023 году и с тех пор стали стандартом для современных LLM.

### 2.3 Swish/SwiGLU заменяет GELU

Ранние архитектуры GPT использовали активационную функцию GELU. Почему теперь используют Swish вместо GELU? Swish (также известная как сигмоидный линейный блок, SiLU) считается вычислительно немного дешевле, и, на мой взгляд, в этом и заключается вся причина. В зависимости от того, на какую статью вы посмотрите, вы обнаружите, что одна функция немного лучше другой с точки зрения производительности моделирования. На мой взгляд, эти небольшие различия, вероятно, лежат в пределах стандартной погрешности, и конкретный результат будет сильно зависеть от тонкой настройки гиперпараметров.

Активационные функции были горячей темой для споров, пока сообщество глубокого обучения более десяти лет назад в основном не остановилось на ReLU. С тех пор исследователи предлагали и пробовали множество вариантов, похожих на ReLU, но с более гладкими кривыми; GELU и Swish (Рисунок 5) — это те из них, что прижились.

![Рисунок 5: Сравнение функций активации Swish и GELU — более гладких версий ReLU.](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-39-40/assets/Figure-05.jpg)

Ранние архитектуры GPT использовали GELU, которая определяется как

$$
\frac{x}{2} \cdot \left[1 + \text{erf}\left(\frac{x}{\sqrt{2}}\right)\right]
$$ 

Здесь $\text{erf}$ (от англ. *error function* — функция ошибок) — это интеграл от гауссовой функции, вычисляемый с помощью полиномиальных приближений, что делает его вычислительно более затратным, чем более простые функции, например, сигмоиду, используемую в Swish ($x * sigmoid(x)$).

На практике Swish вычислительно немного дешевле GELU, и это, вероятно, основная причина, по которой она заменила GELU в большинстве новых моделей. В зависимости от статьи, одна из функций может оказаться несколько лучше с точки зрения качества модели. Но я бы сказал, что эти улучшения часто находятся в пределах погрешности, а победитель будет сильно зависеть от настройки гиперпараметров.

Swish используется в большинстве современных архитектур. Однако GELU не полностью забыта; например, модели Google Gemma по-прежнему используют GELU.

Однако более значимое изменение заключается в том, что сам feed-forward модуль (небольшая многослойная сеть) заменен на его «воротируемый» аналог — GLU (Gated Linear Unit), предложенный в статье 2020 года. Конкретно, 2 полносвязных слоя заменяются на 3, которые используются, как показано на Рисунке 6 ниже.

![Рисунок 6: Сравнение обычного feed-forward слоя с его воротируемыми аналогами SwiGLU и GEGLU.](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-39-40/assets/Figure-06.jpg)

На первый взгляд может показаться, что варианты GEGLU/SwiGLU лучше обычных слоев просто потому, что в них больше параметров из-за дополнительного слоя. Но это обманчиво, потому что на практике весовые матрицы $W$ и $V$ в SwiGLU/GEGLU обычно выбираются в два раза меньше, чем матрица $W_1$ в традиционном feed-forward слое.

Чтобы проиллюстрировать это лучше, рассмотрим конкретные реализации в коде:

![Рисунок 7: Обычный feed-forward модуль (сверху) и вариант SwiGLU (снизу) рядом. Обратите внимание, что функция Swish реализована как «silu» в PyTorch.](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-39-40/assets/Figure-07.jpg)

Предположим, размерность эмбеддинга равна 1024. В случае обычного feed-forward слоя:
*   `fc1`: 1024 × 4096 = 4 194 304 параметра
*   `fc2`: 4096 × 1024 = 4 194 304 параметра
*   Итого: 8 388 608 параметров.

Для варианта GLU:
*   `fc1`: 1024 × 1024 = 1 048 576 параметров
*   `fc2`: 1024 × 1024 = 1 048 576 параметров
*   `fc3`: 1024 × 1024 = 1 048 576 параметров
*   Итого: 3 × 1 048 576 = 3 145 728 параметров.

Таким образом, использование вариантов GLU в итоге приводит к *меньшему* общему количеству параметров, при этом они еще и показывают лучшую производительность. Причина этого в том, что эти варианты обеспечивают дополнительное мультипликативное взаимодействие, что повышает выразительную способность сети (по той же причине глубокие и узкие сети могут превзойти широкие и мелкие при условии качественного обучения).

### 2.4 Mixture-of-Experts вместо единого модуля FeedForward

Помимо обновления модуля feed-forward до SwiGLU, о чём шла речь в предыдущем разделе, в gpt-oss единый feed-forward модуль заменяется на несколько таких модулей, при этом на каждом шаге генерации токена используется лишь подмножество из них. Такой подход известен как «смесь экспертов» (Mixture-of-Experts, MoE) и проиллюстрирован на Рисунке 8 ниже.

![Рисунок 8: Модуль feed-forward заменяется на модуль «смеси экспертов» (MoE).](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-39-40/assets/Figure-08.png)

Таким образом, замена одного feed-forward модуля на несколько (как это реализовано в архитектуре MoE) существенно увеличивает общее количество параметров модели. Однако ключевая хитрость заключается в том, что не все «эксперты» задействуются («активируются») для каждого токена. Вместо этого специальный маршрутизатор (router) выбирает лишь небольшое подмножество экспертов для каждого конкретного токена.

Поскольку одновременно активны лишь несколько экспертов, модули MoE часто называют разреженными (sparse), в отличие от плотных (dense) модулей, которые всегда используют полный набор параметров. При этом большое общее количество параметров, обеспечиваемое архитектурой MoE, повышает ёмкость языковой модели, позволяя ей усваивать больше знаний в процессе обучения. В то же время разреженность сохраняет эффективность вывода (inference), поскольку не все параметры задействуются одновременно.

(Интересный факт: в большинстве моделей MoE веса экспертов составляют более 90 % от общего числа параметров модели.)
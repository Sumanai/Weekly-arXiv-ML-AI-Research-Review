# От GPT-2 к gpt-oss: анализ достижений архитектуры

> И как они выглядят на фоне Qwen3

5 августа, 2025 года OpenAI выпустила новые модели LLM с открытым весом: gpt-oss-120b и gpt-oss-20b — первые полностью открытые модели с момента выхода GPT-2 в 2019 году. И да, благодаря некоторым умным оптимизациям, их можно запускать локально (но об этом чуть позже).

Это первый раз с момента выпуска GPT-2, когда OpenAI делится крупной полностью открытой моделью. Ранние модели GPT показали, как масштабируется архитектура трансформеров. Затем выпуск ChatGPT в 2022 году сделал эти модели мейнстримом, продемонстрировав их практическую пользу для задач письма, получения знаний (а позже и программирования). Теперь же компания поделилась долгожданными весами модели, и архитектура содержит несколько интересных деталей.

Я провёл последние несколько дней, изучая код и технические отчёты, чтобы обобщить самые интересные подробности. (Спустя всего несколько дней после этого OpenAI также анонсировала GPT-5 — я кратко затрону её в контексте моделей gpt-oss в конце статьи.)

Ниже — краткий обзор того, о чём пойдёт речь в статье. Для удобной навигации рекомендую использовать оглавление слева на странице статьи.

- Сравнение архитектуры моделей с GPT-2  
- Оптимизация MXFP4, позволяющая разместить модели gpt-oss на одной видеокарте  
- Компромиссы между шириной и глубиной (gpt-oss vs Qwen3)  
- Внимание, смещения и «поглотители» (attention bias and sinks)  
- Бенчмарки и сравнение с GPT-5  

Надеюсь, вы найдёте эту статью полезной!

## 1. Обзор архитектуры модели

Прежде чем подробно обсуждать архитектуру, начнём с обзора двух моделей — `gpt-oss-20b` и `gpt-oss-120b`, показанных на Рисунке 1 ниже.

![Рисунок 1: Две модели gpt-oss рядом.](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-39-40/assets/Figure-01.png)

Если вы раньше видели схемы современных LLM или читали мою предыдущую статью «Большое сравнение архитектур», вы можете заметить, что на первый взгляд здесь нет ничего принципиально нового или необычного.

Это неудивительно: ведущие разработчики LLM, как правило, используют одну и ту же базовую архитектуру, внося лишь небольшие доработки. Это моё личное предположение, но, думаю, причины в следующем:

- Между лабораториями происходит значительная ротация сотрудников.
- Мы до сих пор не нашли ничего лучше архитектуры трансформера. Хотя существуют state space models и модели диффузии текста, насколько мне известно, никто не доказал, что они работают так же хорошо, как трансформеры, на данном масштабе. (Большинство сравнений, которые я нашёл, сосредоточены только на результатах бенчмарков. До сих пор неясно, насколько хорошо эти модели справляются с реальными многоходовыми задачами письма и программирования. На момент написания статьи самая высокорейтинговая неполностью-трансформерная модель на LM Arena — Jamba, гибрид трансформера и state space модели, занимает 96-е место. *ПРИМЕЧАНИЕ: мне любезно указали, что существует более высокорейтинговая гибридная модель — Hunyuan-TurboS на 22-м месте.*)
- Большая часть улучшений, скорее всего, достигается за счёт данных и тонкой настройки алгоритмов, а не за счёт кардинальных изменений архитектуры.

Тем не менее, в их выборе дизайна есть множество интересных аспектов. Некоторые из них показаны на рисунке выше (другие — нет, но мы обсудим их позже). В оставшейся части статьи я поочерёдно выделю эти особенности и сравню их с другими архитектурами.

Также хочу отметить, что я никоим образом не связан с OpenAI. Моя информация основана исключительно на изучении опубликованного кода модели и технических отчётов. Если вы хотите узнать, как использовать эти модели локально, лучшее место для начала — официальные страницы моделей OpenAI на Hugging Face Hub:

- https://huggingface.co/openai/gpt-oss-20b
- https://huggingface.co/openai/gpt-oss-120b

Модель на 20 млрд параметров (`gpt-oss-20b`) может работать на потребительской видеокарте с 16 ГБ ОЗУ. Модель на 120 млрд параметров (`gpt-oss-120b`) может работать на одной карте NVIDIA H100 с 80 ГБ ОЗУ или на более новом оборудовании. Я вернусь к этому позже, поскольку есть несколько важных оговорок.

---

## 2. Наследие GPT-2

Прежде чем углубиться в сравнение gpt-oss с более современными архитектурами, давайте совершим путешествие во времени и сравним ее бок о бок с GPT-2 (Рисунок 2), чтобы наглядно увидеть, какой путь был пройден.

![Рисунок 2: Сравнение архитектур gpt-oss-20b и GPT-2 XL 1.5B.](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-39-40/assets/Figure-02.jpg)

И gpt-oss, и GPT-2 — это LLM, использующие только декодер и построенные на архитектуре трансформера, представленной в знаменитой статье «Attention Is All You Need» (2017). За прошедшие годы многие детали этой архитектуры эволюционировали.

Впрочем, эти изменения не являются уникальными для gpt-oss. Как мы увидим далее, они встречаются во многих других современных языковых моделях. Поскольку я уже подробно обсуждал многие из этих аспектов в предыдущей статье «Большое сравнение архитектур», я постараюсь быть краткими и сосредоточиться на ключевых моментах.

### 2.1 Отказ от Dropout

Dropout (2012) — это классический метод предотвращения переобучения, который случайным образом «отключает» (то есть обнуляет) часть активаций слоя или оценок внимания (Рисунок 3) во время обучения. Однако в современных больших языковых моделях dropout используется крайне редко, и большинство моделей, вышедших после GPT-2, от него отказались.

![Рисунок 3: Иллюстрация применения dropout к матрице оценок внимания.](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-39-40/assets/Figure-03.png)

Можно предположить, что изначально dropout был использован в GPT-2 как наследие оригинальной архитектуры трансформера. Исследователи, скорее всего, заметили, что он не дает реального улучшения производительности LLM (я наблюдал то же самое в своих небольших экспериментах по воспроизведению GPT-2). Это связано с тем, что LLM обычно обучаются всего за одну эпоху на огромных наборах данных, в отличие от режимов обучения в сотни эпох, для которых dropout изначально был создан. Поскольку LLM видят каждый токен только один раз за всё обучение, риск переобучения невелик.

Что интересно, хотя dropout много лет игнорировался при проектировании архитектур LLM, я нашел исследовательскую статью 2025 года с экспериментами на относительно небольших моделях (Pythia 1.4B), которая подтверждает, что в условиях обучения в одну эпоху dropout приводит к ухудшению итогового качества модели.

### 2.2 RoPE заменяет абсолютные позиционные эмбеддинги

В трансформерных LLM позиционное кодирование необходимо из-за механизма внимания. По умолчанию attention рассматривает входные токены так, как если бы они не имели порядка. В оригинальной архитектуре GPT эту проблему решали абсолютные позиционные эмбеддинги: к вектору токена добавлялся изученный вектор, соответствующий его позиции в последовательности (Рисунок 4).

![Рисунок 4: Иллюстрация абсолютных позиционных эмбеддингов.](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-39-40/assets/Figure-04.jpg)

RoPE (Rotary Position Embedding) предложила другой подход: вместо добавления позиционной информации в виде отдельных векторов, она кодирует позицию путем вращения векторов запроса и ключа, которое зависит от позиции каждого токена. (Идея RoPE элегантна, но ее объяснение — тема сложная, которую я планирую подробно разобрать отдельно.)

Впервые представленные в 2021 году, RoPE получили широкое распространение с выходом оригинальной модели Llama в 2023 году и с тех пор стали стандартом для современных LLM.

### 2.3 Swish/SwiGLU заменяет GELU

Ранние архитектуры GPT использовали активационную функцию GELU. Почему теперь используют Swish вместо GELU? Swish (также известная как сигмоидный линейный блок, SiLU) считается вычислительно немного дешевле, и, на мой взгляд, в этом и заключается вся причина. В зависимости от того, на какую статью вы посмотрите, вы обнаружите, что одна функция немного лучше другой с точки зрения производительности моделирования. На мой взгляд, эти небольшие различия, вероятно, лежат в пределах стандартной погрешности, и конкретный результат будет сильно зависеть от тонкой настройки гиперпараметров.

Активационные функции были горячей темой для споров, пока сообщество глубокого обучения более десяти лет назад в основном не остановилось на ReLU. С тех пор исследователи предлагали и пробовали множество вариантов, похожих на ReLU, но с более гладкими кривыми; GELU и Swish (Рисунок 5) — это те из них, что прижились.

![Рисунок 5: Сравнение функций активации Swish и GELU — более гладких версий ReLU.](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-39-40/assets/Figure-05.jpg)

Ранние архитектуры GPT использовали GELU, которая определяется как

$$
\frac{x}{2} \cdot \left[1 + \text{erf}\left(\frac{x}{\sqrt{2}}\right)\right]
$$ 

Здесь $\text{erf}$ (от англ. *error function* — функция ошибок) — это интеграл от гауссовой функции, вычисляемый с помощью полиномиальных приближений, что делает его вычислительно более затратным, чем более простые функции, например, сигмоиду, используемую в Swish ($x * sigmoid(x)$).

На практике Swish вычислительно немного дешевле GELU, и это, вероятно, основная причина, по которой она заменила GELU в большинстве новых моделей. В зависимости от статьи, одна из функций может оказаться несколько лучше с точки зрения качества модели. Но я бы сказал, что эти улучшения часто находятся в пределах погрешности, а победитель будет сильно зависеть от настройки гиперпараметров.

Swish используется в большинстве современных архитектур. Однако GELU не полностью забыта; например, модели Google Gemma по-прежнему используют GELU.

Однако более значимое изменение заключается в том, что сам feed-forward модуль (небольшая многослойная сеть) заменен на его «воротируемый» аналог — GLU (Gated Linear Unit), предложенный в статье 2020 года. Конкретно, 2 полносвязных слоя заменяются на 3, которые используются, как показано на Рисунке 6 ниже.

![Рисунок 6: Сравнение обычного feed-forward слоя с его воротируемыми аналогами SwiGLU и GEGLU.](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-39-40/assets/Figure-06.jpg)

На первый взгляд может показаться, что варианты GEGLU/SwiGLU лучше обычных слоев просто потому, что в них больше параметров из-за дополнительного слоя. Но это обманчиво, потому что на практике весовые матрицы $W$ и $V$ в SwiGLU/GEGLU обычно выбираются в два раза меньше, чем матрица $W_1$ в традиционном feed-forward слое.

Чтобы проиллюстрировать это лучше, рассмотрим конкретные реализации в коде:

![Рисунок 7: Обычный feed-forward модуль (сверху) и вариант SwiGLU (снизу) рядом. Обратите внимание, что функция Swish реализована как «silu» в PyTorch.](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-39-40/assets/Figure-07.jpg)

Предположим, размерность эмбеддинга равна 1024. В случае обычного feed-forward слоя:
*   `fc1`: 1024 × 4096 = 4 194 304 параметра
*   `fc2`: 4096 × 1024 = 4 194 304 параметра
*   Итого: 8 388 608 параметров.

Для варианта GLU:
*   `fc1`: 1024 × 1024 = 1 048 576 параметров
*   `fc2`: 1024 × 1024 = 1 048 576 параметров
*   `fc3`: 1024 × 1024 = 1 048 576 параметров
*   Итого: 3 × 1 048 576 = 3 145 728 параметров.

Таким образом, использование вариантов GLU в итоге приводит к *меньшему* общему количеству параметров, при этом они еще и показывают лучшую производительность. Причина этого в том, что эти варианты обеспечивают дополнительное мультипликативное взаимодействие, что повышает выразительную способность сети (по той же причине глубокие и узкие сети могут превзойти широкие и мелкие при условии качественного обучения).

### 2.4 Mixture-of-Experts вместо единого модуля FeedForward

Помимо обновления модуля feed-forward до SwiGLU, о чём шла речь в предыдущем разделе, в gpt-oss единый feed-forward модуль заменяется на несколько таких модулей, при этом на каждом шаге генерации токена используется лишь подмножество из них. Такой подход известен как «смесь экспертов» (Mixture-of-Experts, MoE) и проиллюстрирован на Рисунке 8 ниже.

![Рисунок 8: Модуль feed-forward заменяется на модуль «смеси экспертов» (MoE).](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-39-40/assets/Figure-08.png)

Таким образом, замена одного feed-forward модуля на несколько (как это реализовано в архитектуре MoE) существенно увеличивает общее количество параметров модели. Однако ключевая хитрость заключается в том, что не все «эксперты» задействуются («активируются») для каждого токена. Вместо этого специальный маршрутизатор (router) выбирает лишь небольшое подмножество экспертов для каждого конкретного токена.

Поскольку одновременно активны лишь несколько экспертов, модули MoE часто называют разреженными (sparse), в отличие от плотных (dense) модулей, которые всегда используют полный набор параметров. При этом большое общее количество параметров, обеспечиваемое архитектурой MoE, повышает ёмкость языковой модели, позволяя ей усваивать больше знаний в процессе обучения. В то же время разреженность сохраняет эффективность вывода (inference), поскольку не все параметры задействуются одновременно.

(Интересный факт: в большинстве моделей MoE веса экспертов составляют более 90 % от общего числа параметров модели.)

Конечно! Вот перевод английского фрагмента текста на русский язык, стилистически и терминологически согласованный с вашим оригинальным текстом:

### 2.5 Grouped Query Attention вместо Multi-Head Attention

Как упоминалось в моих предыдущих статьях, в последние годы Grouped Query Attention (GQA, «групповое внимание по запросам») стало более эффективной с точки зрения вычислений и количества параметров альтернативой классическому Multi-Head Attention (MHA, «многоголовому вниманию»).

В MHA каждая «голова» имеет собственные проекции ключей и значений. GQA снижает потребление памяти за счёт объединения нескольких голов в группы, которые совместно используют одни и те же проекции ключей и значений.

Например, как показано на Рисунке 9, если у нас есть 2 группы ключей и значений и 4 головы внимания, то головы 1 и 2 могут использовать одну и ту же пару ключ–значение, а головы 3 и 4 — другую. Такая группировка уменьшает общее количество вычислений для ключей и значений, что приводит к снижению объёма памяти и повышению эффективности без заметного ухудшения качества модели, согласно аблационным исследованиям.

![Рисунок 9: Сравнение MHA и GQA. Здесь размер группы равен 2, где пара «ключ-значение» используется совместно двумя запросами.](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-39-40/assets/Figure-09.png)

Таким образом, основная идея GQA — сократить количество «голов» ключей и значений, заставив несколько голов запросов делить одни и те же ключи и значения. Это (1) уменьшает общее число параметров модели и (2) снижает потребление пропускной способности памяти при выводе, поскольку в кэше KV-пар (ключ–значение) хранится и извлекается меньше данных.

(Если вам интересно, как GQA выглядит в коде, см. моё руководство по преобразованию GPT-2 в Llama 3 — там приведена версия без KV-кэша, а также мой вариант с KV-кэшем.)

Хотя GQA в первую очередь служит инструментом повышения вычислительной эффективности по сравнению с MHA, аблационные исследования (например, в оригинальной статье про GQA и в статье про Llama 2) показывают, что по качеству моделирования она сопоставима со стандартным MHA.

### 2.6 Внимание с подвижным окном (Sliding Window Attention)

Внимание с подвижным окном (Рисунок 10 ниже) впервые было предложено в статье LongFormer (2020) и позже получило широкое распространение благодаря Mistral. Примечательно, что в gpt-oss оно применяется в каждом втором слое. Можно рассматривать его как вариант многоголового внимания (а в данном случае — Grouped Query Attention), в котором контекст внимания ограничен небольшим окном, что снижает как объём памяти, так и вычислительные затраты.

![Рисунок 10: Сравнение обычного внимания (слева) и внимания с подвижным окном (справа).](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-39-40/assets/Figure-10.jpg)

Конкретно, gpt-oss чередует слои GQA с полным доступом ко всему контексту и слои GQA с подвижным окном, ограниченным 128 токенами.

Как я уже обсуждал в предыдущей статье, Gemma 2 (2024) использовала аналогичное соотношение 1:1. А Gemma 3, вышедшая ранее в этом году, пошла ещё дальше и перешла на соотношение 5:1 — то есть только один слой с полным вниманием приходится на каждые пять слоёв с локальным (оконным) вниманием.

Согласно аблационным исследованиям в рамках проекта Gemma, использование внимания с подвижным окном практически не влияет на качество моделирования, как показано на рисунке ниже. Стоит отметить, что размер окна в Gemma 2 составлял 4096 токенов, а в Gemma 3 был уменьшен до 1024. В gpt-oss же окно составляет всего 128 токенов — что удивительно мало.

И в качестве интересного факта: в официальной анонсирующей статье отмечается, что внимание с подвижным окном, похоже, уже использовалось в GPT-3:

> «Модели используют чередующиеся плотные и локально-полосатые разреженные паттерны внимания, аналогичные GPT-3».

Кто бы мог подумать! Я перечитал оригинальную статью про GPT-3, и там действительно упоминалось:

> «Мы используем ту же модель и архитектуру, что и в GPT-2 [RWC+19], включая модифицированную инициализацию, предварительную нормализацию и обратимую токенизацию, описанные в той работе, за исключением того, что в слоях трансформера мы применяем чередующиеся плотные и локально-полосатые разреженные паттерны внимания, аналогичные Sparse Transformer [CGRS19].»

### 2.7 RMSNorm вместо LayerNorm

Наконец, последнее небольшое улучшение по сравнению с GPT-2 — замена LayerNorm (2016) на RMSNorm (2019), что стало общей тенденцией в последние годы.

Подобно замене GELU на Swish и SwiGLU, RMSNorm — это ещё одно небольшое, но разумное улучшение эффективности. RMSNorm, как и LayerNorm, предназначен для нормализации активаций слоя, как показано на Рисунке 11 ниже.

Возможно, вы помните, что не так давно стандартом де-факто была BatchNorm. Однако она утратила популярность, главным образом потому, что её сложно эффективно распараллеливать (из-за необходимости вычислять статистики по батчу — среднее и дисперсию) и она плохо работает при малых размерах батчей.

![Рисунок 11: Сравнение LayerNorm (слева) и RMSNorm (справа) на примере небольшого линейного слоя.](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-39-40/assets/Figure-11.jpg)

Как видно на Рисунке 11, и LayerNorm, и RMSNorm масштабируют выходы слоя, приводя их к разумному диапазону значений.

LayerNorm вычитает среднее значение и делит на стандартное отклонение, чтобы выходы слоя имели нулевое среднее и единичную дисперсию (дисперсия = 1, стандартное отклонение = 1).

RMSNorm делит входы на корень из среднего квадрата (root-mean-square). Это масштабирует активации до сопоставимой величины, но не принуждает их к нулевому среднему или единичной дисперсии. В приведённом примере среднее значение равно 0.77, а дисперсия — 0.41.

Обе нормализации стабилизируют масштаб активаций и улучшают обучаемость, однако RMSNorm чаще предпочтителен в крупномасштабных LLM, потому что он дешевле в вычислениях. В отличие от LayerNorm, RMSNorm не содержит смещающего (bias) члена и заменяет дорогие операции вычисления среднего и дисперсии на одну операцию вычисления среднеквадратичного значения. Это сокращает количество межпризнаковых редукций с двух до одной, что снижает коммуникационные накладные расходы на GPU и повышает эффективность обучения.

На Рисунке 12 показано, как это выглядит в коде:

![Рисунок 12: Реализации LayerNorm и RMSNorm в коде, демонстрирующие, что RMSNorm вычислительно проще.](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-39-40/assets/Figure-12.jpg)